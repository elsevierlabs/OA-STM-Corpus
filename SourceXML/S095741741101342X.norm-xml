<?xml version="1.0" encoding="UTF-8"?><!-- Normalized for easier text mining. --><xocs:doc xmlns:xocs="http://www.elsevier.com/xml/xocs/dtd" xmlns:xs="http://www.w3.org/2001/XMLSchema" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http://www.elsevier.com/xml/ja/dtd" xmlns:ja="http://www.elsevier.com/xml/ja/dtd" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:tb="http://www.elsevier.com/xml/common/table/dtd" xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/dtd" xmlns:ce="http://www.elsevier.com/xml/common/dtd" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:cals="http://www.elsevier.com/xml/common/cals/dtd" xsi:schemaLocation="http://www.elsevier.com/xml/xocs/dtd http://schema.elsevier.com/dtds/document/fulltext/xcr/xocs-article.xsd"><xocs:meta><xocs:content-family>serial</xocs:content-family><xocs:content-type>JL</xocs:content-type><xocs:cid>271506</xocs:cid><xocs:srctitle>Expert Systems with Applications</xocs:srctitle><xocs:normalized-srctitle>EXPERTSYSTEMSAPPLICATIONS</xocs:normalized-srctitle><xocs:orig-load-date>2011-09-08</xocs:orig-load-date><xocs:ew-transaction-id>2013-05-16T21:00:57</xocs:ew-transaction-id><xocs:eid>1-s2.0-S095741741101342X</xocs:eid><xocs:pii-formatted>S0957-4174(11)01342-X</xocs:pii-formatted><xocs:pii-unformatted>S095741741101342X</xocs:pii-unformatted><xocs:doi>10.1016/j.eswa.2011.09.033</xocs:doi><xocs:item-stage>S300</xocs:item-stage><xocs:item-version-number>S300.1</xocs:item-version-number><xocs:item-weight>FULL-TEXT</xocs:item-weight><xocs:hub-eid>1-s2.0-S0957417411X0012X</xocs:hub-eid><xocs:timestamp>2014-03-10T10:13:19.532993-04:00</xocs:timestamp><xocs:issns><xocs:issn-primary-formatted>0957-4174</xocs:issn-primary-formatted><xocs:issn-primary-unformatted>09574174</xocs:issn-primary-unformatted></xocs:issns><xocs:sponsored-access-type>UNLIMITED</xocs:sponsored-access-type><xocs:funding-body-id>NONE</xocs:funding-body-id><xocs:crossmark>false</xocs:crossmark><xocs:vol-first>39</xocs:vol-first><xocs:iss-first>3</xocs:iss-first><xocs:vol-iss-suppl-text>Volume 39, Issue 3</xocs:vol-iss-suppl-text><xocs:sort-order>129</xocs:sort-order><xocs:first-fp>3446</xocs:first-fp><xocs:last-lp>3453</xocs:last-lp><xocs:pages><xocs:first-page>3446</xocs:first-page><xocs:last-page>3453</xocs:last-page></xocs:pages><xocs:cover-date-orig><xocs:start-date>20120215</xocs:start-date></xocs:cover-date-orig><xocs:cover-date-text>15 February 2012</xocs:cover-date-text><xocs:cover-date-start>2012-02-15</xocs:cover-date-start><xocs:cover-date-year>2012</xocs:cover-date-year><xocs:hub-sec><xocs:hub-sec-title>Regular Articles</xocs:hub-sec-title></xocs:hub-sec><xocs:document-type>article</xocs:document-type><xocs:document-subtype>fla</xocs:document-subtype><xocs:copyright-line>Copyright © 2011 Published by Elsevier Ltd.</xocs:copyright-line><xocs:normalized-article-title>EXPERIMENTALCOMPARISONCLASSIFICATIONALGORITHMSFORIMBALANCEDCREDITSCORINGDATASETS</xocs:normalized-article-title><xocs:normalized-first-auth-surname>BROWN</xocs:normalized-first-auth-surname><xocs:normalized-first-auth-initial>I</xocs:normalized-first-auth-initial><xocs:item-toc><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>1</xocs:item-toc-label><xocs:item-toc-section-title>Introduction</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>2</xocs:item-toc-label><xocs:item-toc-section-title>Literature review</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>3</xocs:item-toc-label><xocs:item-toc-section-title>Overview of classification techniques</xocs:item-toc-section-title><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>3.1</xocs:item-toc-label><xocs:item-toc-section-title>Logistic regression</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>3.2</xocs:item-toc-label><xocs:item-toc-section-title>Linear and quadratic discriminant analysis</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>3.3</xocs:item-toc-label><xocs:item-toc-section-title>Neural networks (Multi-layer perceptron)</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>3.4</xocs:item-toc-label><xocs:item-toc-section-title>Least square support vector machines (LS-SVMs)</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>3.5</xocs:item-toc-label><xocs:item-toc-section-title>C4.5. decision trees</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>3.6</xocs:item-toc-label><xocs:item-toc-section-title><ce:italic>k</ce:italic>-NN (memory based reasoning)</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>3.7</xocs:item-toc-label><xocs:item-toc-section-title>Random forests</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>3.8</xocs:item-toc-label><xocs:item-toc-section-title>Gradient boosting</xocs:item-toc-section-title></xocs:item-toc-entry></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>4</xocs:item-toc-label><xocs:item-toc-section-title>Experimental set-up and data sets</xocs:item-toc-section-title><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>4.1</xocs:item-toc-label><xocs:item-toc-section-title>Data set characteristics</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>4.2</xocs:item-toc-label><xocs:item-toc-section-title>Re-sampling setup and performance metrics</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>4.3</xocs:item-toc-label><xocs:item-toc-section-title>Parameter tuning and input selection</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>4.4</xocs:item-toc-label><xocs:item-toc-section-title>Statistical comparison of classifiers</xocs:item-toc-section-title></xocs:item-toc-entry></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>5</xocs:item-toc-label><xocs:item-toc-section-title>Results and discussion</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>6</xocs:item-toc-label><xocs:item-toc-section-title>Conclusions and recommendations for further work</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:acknowledgment"><xocs:item-toc-section-title>Acknowledgements</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:bibliography"><xocs:item-toc-section-title>References</xocs:item-toc-section-title></xocs:item-toc-entry></xocs:item-toc><xocs:references><xocs:ref-info refid="b0005"><xocs:ref-normalized-surname>ALTMAN</xocs:ref-normalized-surname><xocs:ref-pub-year>1968</xocs:ref-pub-year><xocs:ref-first-fp>589</xocs:ref-first-fp><xocs:ref-last-lp>609</xocs:ref-last-lp><xocs:ref-normalized-initial>E</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="b0010"><xocs:ref-normalized-surname>ALTMAN</xocs:ref-normalized-surname><xocs:ref-pub-year>1994</xocs:ref-pub-year><xocs:ref-first-fp>505</xocs:ref-first-fp><xocs:ref-last-lp>529</xocs:ref-last-lp><xocs:ref-normalized-initial>E</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="b0015"><xocs:ref-normalized-surname>ARMINGER</xocs:ref-normalized-surname><xocs:ref-pub-year>1997</xocs:ref-pub-year><xocs:ref-first-fp>293</xocs:ref-first-fp><xocs:ref-last-lp>310</xocs:ref-last-lp><xocs:ref-normalized-initial>G</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="b0020"><xocs:ref-normalized-surname>BAESENS</xocs:ref-normalized-surname><xocs:ref-pub-year>2003</xocs:ref-pub-year><xocs:ref-first-fp>627</xocs:ref-first-fp><xocs:ref-last-lp>635</xocs:ref-last-lp><xocs:ref-normalized-initial>B</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="b0025"><xocs:ref-normalized-surname>BATISTA</xocs:ref-normalized-surname><xocs:ref-pub-year>2004</xocs:ref-pub-year><xocs:ref-first-fp>20</xocs:ref-first-fp><xocs:ref-last-lp>29</xocs:ref-last-lp><xocs:ref-normalized-initial>G</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="b0030"/><xocs:ref-info refid="b0035"><xocs:ref-normalized-surname>BISHOP</xocs:ref-normalized-surname><xocs:ref-pub-year>1995</xocs:ref-pub-year><xocs:ref-normalized-initial>C</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>NEURALNETWORKSFORPATTERNRECOGNITION</xocs:ref-normalized-srctitle></xocs:ref-info><xocs:ref-info refid="b0040"><xocs:ref-normalized-surname>BREIMAN</xocs:ref-normalized-surname><xocs:ref-pub-year>2001</xocs:ref-pub-year><xocs:ref-first-fp>5</xocs:ref-first-fp><xocs:ref-last-lp>32</xocs:ref-last-lp><xocs:ref-normalized-initial>L</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="b0045"><xocs:ref-normalized-surname>CHATTERJEE</xocs:ref-normalized-surname><xocs:ref-pub-year>1970</xocs:ref-pub-year><xocs:ref-first-fp>50</xocs:ref-first-fp><xocs:ref-last-lp>154</xocs:ref-last-lp><xocs:ref-normalized-initial>S</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="b0050"><xocs:ref-normalized-surname>CHAWLA</xocs:ref-normalized-surname><xocs:ref-pub-year>2002</xocs:ref-pub-year><xocs:ref-first-fp>321</xocs:ref-first-fp><xocs:ref-last-lp>357</xocs:ref-last-lp><xocs:ref-normalized-initial>N</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="b0055"><xocs:ref-normalized-surname>DELONG</xocs:ref-normalized-surname><xocs:ref-pub-year>1988</xocs:ref-pub-year><xocs:ref-first-fp>837</xocs:ref-first-fp><xocs:ref-last-lp>845</xocs:ref-last-lp><xocs:ref-normalized-initial>E</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="b0060"><xocs:ref-normalized-surname>DEMSAR</xocs:ref-normalized-surname><xocs:ref-pub-year>2006</xocs:ref-pub-year><xocs:ref-first-fp>1</xocs:ref-first-fp><xocs:ref-last-lp>30</xocs:ref-last-lp><xocs:ref-normalized-initial>J</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="b0065"><xocs:ref-normalized-surname>DESAI</xocs:ref-normalized-surname><xocs:ref-pub-year>1996</xocs:ref-pub-year><xocs:ref-first-fp>24</xocs:ref-first-fp><xocs:ref-last-lp>37</xocs:ref-last-lp><xocs:ref-normalized-initial>V</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="b0070"><xocs:ref-normalized-surname>FRIEDMAN</xocs:ref-normalized-surname><xocs:ref-pub-year>1940</xocs:ref-pub-year><xocs:ref-first-fp>86</xocs:ref-first-fp><xocs:ref-last-lp>92</xocs:ref-last-lp><xocs:ref-normalized-initial>M</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="b0075"><xocs:ref-normalized-surname>FRIEDMAN</xocs:ref-normalized-surname><xocs:ref-pub-year>2001</xocs:ref-pub-year><xocs:ref-first-fp>1189</xocs:ref-first-fp><xocs:ref-last-lp>1232</xocs:ref-last-lp><xocs:ref-normalized-initial>J</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="b0080"><xocs:ref-normalized-surname>FRIEDMAN</xocs:ref-normalized-surname><xocs:ref-pub-year>2002</xocs:ref-pub-year><xocs:ref-first-fp>367</xocs:ref-first-fp><xocs:ref-last-lp>378</xocs:ref-last-lp><xocs:ref-normalized-initial>J</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="b0085"><xocs:ref-normalized-surname>HASTIE</xocs:ref-normalized-surname><xocs:ref-pub-year>2001</xocs:ref-pub-year><xocs:ref-normalized-initial>T</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>ELEMENTSSTATISTICALLEARNINGDATAMININGINFERENCEPREDICTION</xocs:ref-normalized-srctitle></xocs:ref-info><xocs:ref-info refid="b0090"><xocs:ref-normalized-surname>HENLEY</xocs:ref-normalized-surname><xocs:ref-pub-year>1997</xocs:ref-pub-year><xocs:ref-first-fp>305</xocs:ref-first-fp><xocs:ref-last-lp>321</xocs:ref-last-lp><xocs:ref-normalized-initial>W</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="b0095"><xocs:ref-normalized-surname>HOSMER</xocs:ref-normalized-surname><xocs:ref-pub-year>2000</xocs:ref-pub-year><xocs:ref-normalized-initial>D</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>APPLIEDLOGISTICREGRESSION</xocs:ref-normalized-srctitle></xocs:ref-info><xocs:ref-info refid="b0100"/><xocs:ref-info refid="b0105"><xocs:ref-normalized-surname>LESSMANN</xocs:ref-normalized-surname><xocs:ref-pub-year>2008</xocs:ref-pub-year><xocs:ref-first-fp>485</xocs:ref-first-fp><xocs:ref-last-lp>496</xocs:ref-last-lp><xocs:ref-normalized-initial>S</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="b0110"/><xocs:ref-info refid="b0115"><xocs:ref-normalized-surname>PROVOST</xocs:ref-normalized-surname><xocs:ref-pub-year>1999</xocs:ref-pub-year><xocs:ref-normalized-initial>F</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGSFIFTHINTERNATIONALCONFERENCEKNOWLEDGEDISCOVERYDATAMINING</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>EFFICIENTPROGRESSIVESAMPLING</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="b0120"><xocs:ref-normalized-surname>QUINLAN</xocs:ref-normalized-surname><xocs:ref-pub-year>1993</xocs:ref-pub-year><xocs:ref-normalized-initial>J</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>C45PROGRAMSFORMACHINELEARNING</xocs:ref-normalized-srctitle></xocs:ref-info><xocs:ref-info refid="b0125"><xocs:ref-normalized-surname>STEENACKERS</xocs:ref-normalized-surname><xocs:ref-pub-year>1989</xocs:ref-pub-year><xocs:ref-first-fp>31</xocs:ref-first-fp><xocs:ref-last-lp>34</xocs:ref-last-lp><xocs:ref-normalized-initial>A</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="b0130"><xocs:ref-normalized-surname>SUYKENS</xocs:ref-normalized-surname><xocs:ref-pub-year>2002</xocs:ref-pub-year><xocs:ref-normalized-initial>J</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>LEASTSQUARESSUPPORTVECTORMACHINES</xocs:ref-normalized-srctitle></xocs:ref-info><xocs:ref-info refid="b0135"><xocs:ref-normalized-surname>VANDERBURGT</xocs:ref-normalized-surname><xocs:ref-pub-year>2007</xocs:ref-pub-year><xocs:ref-normalized-initial>M</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>CALIBRATINGLOWDEFAULTPORTFOLIOSUSINGCUMULATIVEACCURACYPROFILE</xocs:ref-normalized-srctitle></xocs:ref-info><xocs:ref-info refid="b0140"><xocs:ref-normalized-surname>WEISS</xocs:ref-normalized-surname><xocs:ref-pub-year>2003</xocs:ref-pub-year><xocs:ref-first-fp>315</xocs:ref-first-fp><xocs:ref-last-lp>354</xocs:ref-last-lp><xocs:ref-normalized-initial>G</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="b0145"><xocs:ref-normalized-surname>WEST</xocs:ref-normalized-surname><xocs:ref-pub-year>2000</xocs:ref-pub-year><xocs:ref-first-fp>1131</xocs:ref-first-fp><xocs:ref-last-lp>1152</xocs:ref-last-lp><xocs:ref-normalized-initial>D</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="b0150"><xocs:ref-normalized-surname>WIGINTON</xocs:ref-normalized-surname><xocs:ref-pub-year>1980</xocs:ref-pub-year><xocs:ref-first-fp>757</xocs:ref-first-fp><xocs:ref-last-lp>770</xocs:ref-last-lp><xocs:ref-normalized-initial>J</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="b0155"><xocs:ref-normalized-surname>WITTEN</xocs:ref-normalized-surname><xocs:ref-pub-year>2005</xocs:ref-pub-year><xocs:ref-normalized-initial>I</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>DATAMININGPRACTICALMACHINELEARNINGTOOLSTECHNIQUES</xocs:ref-normalized-srctitle></xocs:ref-info><xocs:ref-info refid="b0160"><xocs:ref-normalized-surname>YANG</xocs:ref-normalized-surname><xocs:ref-pub-year>2007</xocs:ref-pub-year><xocs:ref-first-fp>1521</xocs:ref-first-fp><xocs:ref-last-lp>1536</xocs:ref-last-lp><xocs:ref-normalized-initial>Y</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="b0165"><xocs:ref-normalized-surname>YOBAS</xocs:ref-normalized-surname><xocs:ref-pub-year>2000</xocs:ref-pub-year><xocs:ref-first-fp>111</xocs:ref-first-fp><xocs:ref-last-lp>125</xocs:ref-last-lp><xocs:ref-normalized-initial>M</xocs:ref-normalized-initial></xocs:ref-info></xocs:references><xocs:attachment-metadata-doc><xocs:attachment-set-type>item</xocs:attachment-set-type><xocs:pii-formatted>S0957-4174(11)01342-X</xocs:pii-formatted><xocs:pii-unformatted>S095741741101342X</xocs:pii-unformatted><xocs:eid>1-s2.0-S095741741101342X</xocs:eid><xocs:doi>10.1016/j.eswa.2011.09.033</xocs:doi><xocs:cid>271506</xocs:cid><xocs:timestamp>2013-05-16T23:21:50.185467-04:00</xocs:timestamp><xocs:path>/271506/1-s2.0-S0957417411X0012X/1-s2.0-S095741741101342X/</xocs:path><xocs:cover-date-start>2012-02-15</xocs:cover-date-start><xocs:sponsored-access-type>UNLIMITED</xocs:sponsored-access-type><xocs:funding-body-id>NONE</xocs:funding-body-id><xocs:attachments><xocs:web-pdf><xocs:attachment-eid>1-s2.0-S095741741101342X-main.pdf</xocs:attachment-eid><xocs:filename>main.pdf</xocs:filename><xocs:extension>pdf</xocs:extension><xocs:pdf-optimized>true</xocs:pdf-optimized><xocs:filesize>519257</xocs:filesize><xocs:web-pdf-purpose>MAIN</xocs:web-pdf-purpose><xocs:web-pdf-page-count>8</xocs:web-pdf-page-count><xocs:web-pdf-images><xocs:web-pdf-image><xocs:attachment-eid>1-s2.0-S095741741101342X-main_1.png</xocs:attachment-eid><xocs:filename>main_1.png</xocs:filename><xocs:extension>png</xocs:extension><xocs:filesize>132249</xocs:filesize><xocs:pixel-height>849</xocs:pixel-height><xocs:pixel-width>656</xocs:pixel-width><xocs:attachment-type>IMAGE-WEB-PDF</xocs:attachment-type><xocs:pdf-page-num>1</xocs:pdf-page-num></xocs:web-pdf-image></xocs:web-pdf-images></xocs:web-pdf><xocs:attachment><xocs:attachment-eid>1-s2.0-S095741741101342X-si22.gif</xocs:attachment-eid><xocs:file-basename>si22</xocs:file-basename><xocs:filename>si22.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>1121</xocs:filesize><xocs:pixel-height>42</xocs:pixel-height><xocs:pixel-width>175</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S095741741101342X-si18.gif</xocs:attachment-eid><xocs:file-basename>si18</xocs:file-basename><xocs:filename>si18.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>2507</xocs:filesize><xocs:pixel-height>52</xocs:pixel-height><xocs:pixel-width>471</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S095741741101342X-si16.gif</xocs:attachment-eid><xocs:file-basename>si16</xocs:file-basename><xocs:filename>si16.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>1343</xocs:filesize><xocs:pixel-height>18</xocs:pixel-height><xocs:pixel-width>348</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S095741741101342X-si15.gif</xocs:attachment-eid><xocs:file-basename>si15</xocs:file-basename><xocs:filename>si15.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>1350</xocs:filesize><xocs:pixel-height>36</xocs:pixel-height><xocs:pixel-width>341</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S095741741101342X-si14.gif</xocs:attachment-eid><xocs:file-basename>si14</xocs:file-basename><xocs:filename>si14.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>1339</xocs:filesize><xocs:pixel-height>18</xocs:pixel-height><xocs:pixel-width>291</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S095741741101342X-si12.gif</xocs:attachment-eid><xocs:file-basename>si12</xocs:file-basename><xocs:filename>si12.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>1505</xocs:filesize><xocs:pixel-height>52</xocs:pixel-height><xocs:pixel-width>245</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S095741741101342X-si11.gif</xocs:attachment-eid><xocs:file-basename>si11</xocs:file-basename><xocs:filename>si11.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>1026</xocs:filesize><xocs:pixel-height>21</xocs:pixel-height><xocs:pixel-width>292</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S095741741101342X-si10.gif</xocs:attachment-eid><xocs:file-basename>si10</xocs:file-basename><xocs:filename>si10.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>1484</xocs:filesize><xocs:pixel-height>47</xocs:pixel-height><xocs:pixel-width>251</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S095741741101342X-si9.gif</xocs:attachment-eid><xocs:file-basename>si9</xocs:file-basename><xocs:filename>si9.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>1253</xocs:filesize><xocs:pixel-height>52</xocs:pixel-height><xocs:pixel-width>189</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S095741741101342X-si7.gif</xocs:attachment-eid><xocs:file-basename>si7</xocs:file-basename><xocs:filename>si7.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>1318</xocs:filesize><xocs:pixel-height>52</xocs:pixel-height><xocs:pixel-width>199</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S095741741101342X-si5.gif</xocs:attachment-eid><xocs:file-basename>si5</xocs:file-basename><xocs:filename>si5.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>3119</xocs:filesize><xocs:pixel-height>76</xocs:pixel-height><xocs:pixel-width>423</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S095741741101342X-si4.gif</xocs:attachment-eid><xocs:file-basename>si4</xocs:file-basename><xocs:filename>si4.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>1003</xocs:filesize><xocs:pixel-height>42</xocs:pixel-height><xocs:pixel-width>148</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S095741741101342X-si1.gif</xocs:attachment-eid><xocs:file-basename>si1</xocs:file-basename><xocs:filename>si1.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>1257</xocs:filesize><xocs:pixel-height>34</xocs:pixel-height><xocs:pixel-width>250</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S095741741101342X-si8.gif</xocs:attachment-eid><xocs:file-basename>si8</xocs:file-basename><xocs:filename>si8.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>534</xocs:filesize><xocs:pixel-height>22</xocs:pixel-height><xocs:pixel-width>102</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S095741741101342X-si6.gif</xocs:attachment-eid><xocs:file-basename>si6</xocs:file-basename><xocs:filename>si6.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>283</xocs:filesize><xocs:pixel-height>22</xocs:pixel-height><xocs:pixel-width>24</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S095741741101342X-si3.gif</xocs:attachment-eid><xocs:file-basename>si3</xocs:file-basename><xocs:filename>si3.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>609</xocs:filesize><xocs:pixel-height>17</xocs:pixel-height><xocs:pixel-width>107</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S095741741101342X-si21.gif</xocs:attachment-eid><xocs:file-basename>si21</xocs:file-basename><xocs:filename>si21.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>265</xocs:filesize><xocs:pixel-height>19</xocs:pixel-height><xocs:pixel-width>20</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S095741741101342X-si20.gif</xocs:attachment-eid><xocs:file-basename>si20</xocs:file-basename><xocs:filename>si20.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>265</xocs:filesize><xocs:pixel-height>19</xocs:pixel-height><xocs:pixel-width>20</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S095741741101342X-si2.gif</xocs:attachment-eid><xocs:file-basename>si2</xocs:file-basename><xocs:filename>si2.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>510</xocs:filesize><xocs:pixel-height>17</xocs:pixel-height><xocs:pixel-width>88</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S095741741101342X-si19.gif</xocs:attachment-eid><xocs:file-basename>si19</xocs:file-basename><xocs:filename>si19.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>213</xocs:filesize><xocs:pixel-height>21</xocs:pixel-height><xocs:pixel-width>13</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S095741741101342X-si17.gif</xocs:attachment-eid><xocs:file-basename>si17</xocs:file-basename><xocs:filename>si17.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>631</xocs:filesize><xocs:pixel-height>19</xocs:pixel-height><xocs:pixel-width>104</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S095741741101342X-si13.gif</xocs:attachment-eid><xocs:file-basename>si13</xocs:file-basename><xocs:filename>si13.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>790</xocs:filesize><xocs:pixel-height>19</xocs:pixel-height><xocs:pixel-width>156</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S095741741101342X-gr1.jpg</xocs:attachment-eid><xocs:file-basename>gr1</xocs:file-basename><xocs:filename>gr1.jpg</xocs:filename><xocs:extension>jpg</xocs:extension><xocs:filesize>27717</xocs:filesize><xocs:pixel-height>376</xocs:pixel-height><xocs:pixel-width>377</xocs:pixel-width><xocs:attachment-type>IMAGE-DOWNSAMPLED</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S095741741101342X-gr5.jpg</xocs:attachment-eid><xocs:file-basename>gr5</xocs:file-basename><xocs:filename>gr5.jpg</xocs:filename><xocs:extension>jpg</xocs:extension><xocs:filesize>21504</xocs:filesize><xocs:pixel-height>206</xocs:pixel-height><xocs:pixel-width>578</xocs:pixel-width><xocs:attachment-type>IMAGE-DOWNSAMPLED</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S095741741101342X-gr4.jpg</xocs:attachment-eid><xocs:file-basename>gr4</xocs:file-basename><xocs:filename>gr4.jpg</xocs:filename><xocs:extension>jpg</xocs:extension><xocs:filesize>20988</xocs:filesize><xocs:pixel-height>201</xocs:pixel-height><xocs:pixel-width>573</xocs:pixel-width><xocs:attachment-type>IMAGE-DOWNSAMPLED</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S095741741101342X-gr3.jpg</xocs:attachment-eid><xocs:file-basename>gr3</xocs:file-basename><xocs:filename>gr3.jpg</xocs:filename><xocs:extension>jpg</xocs:extension><xocs:filesize>24129</xocs:filesize><xocs:pixel-height>206</xocs:pixel-height><xocs:pixel-width>573</xocs:pixel-width><xocs:attachment-type>IMAGE-DOWNSAMPLED</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S095741741101342X-gr2.jpg</xocs:attachment-eid><xocs:file-basename>gr2</xocs:file-basename><xocs:filename>gr2.jpg</xocs:filename><xocs:extension>jpg</xocs:extension><xocs:filesize>21663</xocs:filesize><xocs:pixel-height>212</xocs:pixel-height><xocs:pixel-width>576</xocs:pixel-width><xocs:attachment-type>IMAGE-DOWNSAMPLED</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S095741741101342X-gr1.sml</xocs:attachment-eid><xocs:file-basename>gr1</xocs:file-basename><xocs:filename>gr1.sml</xocs:filename><xocs:extension>sml</xocs:extension><xocs:filesize>2680</xocs:filesize><xocs:pixel-height>163</xocs:pixel-height><xocs:pixel-width>164</xocs:pixel-width><xocs:attachment-type>IMAGE-THUMBNAIL</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S095741741101342X-gr5.sml</xocs:attachment-eid><xocs:file-basename>gr5</xocs:file-basename><xocs:filename>gr5.sml</xocs:filename><xocs:extension>sml</xocs:extension><xocs:filesize>2646</xocs:filesize><xocs:pixel-height>78</xocs:pixel-height><xocs:pixel-width>219</xocs:pixel-width><xocs:attachment-type>IMAGE-THUMBNAIL</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S095741741101342X-gr4.sml</xocs:attachment-eid><xocs:file-basename>gr4</xocs:file-basename><xocs:filename>gr4.sml</xocs:filename><xocs:extension>sml</xocs:extension><xocs:filesize>2650</xocs:filesize><xocs:pixel-height>77</xocs:pixel-height><xocs:pixel-width>219</xocs:pixel-width><xocs:attachment-type>IMAGE-THUMBNAIL</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S095741741101342X-gr3.sml</xocs:attachment-eid><xocs:file-basename>gr3</xocs:file-basename><xocs:filename>gr3.sml</xocs:filename><xocs:extension>sml</xocs:extension><xocs:filesize>2832</xocs:filesize><xocs:pixel-height>79</xocs:pixel-height><xocs:pixel-width>219</xocs:pixel-width><xocs:attachment-type>IMAGE-THUMBNAIL</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S095741741101342X-gr2.sml</xocs:attachment-eid><xocs:file-basename>gr2</xocs:file-basename><xocs:filename>gr2.sml</xocs:filename><xocs:extension>sml</xocs:extension><xocs:filesize>2807</xocs:filesize><xocs:pixel-height>81</xocs:pixel-height><xocs:pixel-width>219</xocs:pixel-width><xocs:attachment-type>IMAGE-THUMBNAIL</xocs:attachment-type></xocs:attachment></xocs:attachments></xocs:attachment-metadata-doc><xocs:refkeys><xocs:refkey3>BROWNX2012X3446</xocs:refkey3><xocs:refkey4lp>BROWNX2012X3446X3453</xocs:refkey4lp><xocs:refkey4ai>BROWNX2012X3446XI</xocs:refkey4ai><xocs:refkey5>BROWNX2012X3446X3453XI</xocs:refkey5></xocs:refkeys><xocs:open-access><xocs:oa-access-effective-date>2013-07-16T22:54:29Z</xocs:oa-access-effective-date><xocs:oa-article-status is-open-access="1" is-open-archive="0">Full</xocs:oa-article-status><xocs:oa-sponsor><xocs:oa-sponsor-type>Other</xocs:oa-sponsor-type></xocs:oa-sponsor><xocs:oa-user-license>http://creativecommons.org/licenses/by/3.0/</xocs:oa-user-license></xocs:open-access></xocs:meta><xocs:serial-item><article docsubtype="fla" xml:lang="en" version="5.1"><item-info><jid>ESWA</jid><aid>6947</aid><ce:pii>S0957-4174(11)01342-X</ce:pii><ce:doi>10.1016/j.eswa.2011.09.033</ce:doi><ce:copyright type="unknown" year="2011"/></item-info><ce:floats><ce:figure id="f0005"><ce:label>Fig. 1</ce:label><ce:caption><ce:simple-para id="sp020" view="all">Example ROC curve.</ce:simple-para></ce:caption><ce:link locator="gr1"/></ce:figure><ce:figure id="f0010"><ce:label>Fig. 2</ce:label><ce:caption><ce:simple-para id="sp025" view="all">AR comparison at a 70/30% split of good/bad observations.</ce:simple-para></ce:caption><ce:link locator="gr2"/></ce:figure><ce:figure id="f0015"><ce:label>Fig. 3</ce:label><ce:caption><ce:simple-para id="sp030" view="all">AR comparison at an 85/15% split of good/bad observations.</ce:simple-para></ce:caption><ce:link locator="gr3"/></ce:figure><ce:figure id="f0020"><ce:label>Fig. 4</ce:label><ce:caption><ce:simple-para id="sp035" view="all">AR comparison at a 90/10% split of good/bad observations.</ce:simple-para></ce:caption><ce:link locator="gr4"/></ce:figure><ce:figure id="f0025"><ce:label>Fig. 5</ce:label><ce:caption><ce:simple-para id="sp040" view="all">AR comparison at a 99/1% split of good/bad observations.</ce:simple-para></ce:caption><ce:link locator="gr5"/></ce:figure><ce:table xmlns="http://www.elsevier.com/xml/common/cals/dtd" frame="topbot" id="t0005" rowsep="0" colsep="0"><ce:label>Table 1</ce:label><ce:caption><ce:simple-para id="sp045" view="all">Credit scoring techniques and their applications.</ce:simple-para></ce:caption><tgroup cols="2"><colspec colname="col1" align="left"/><colspec colname="col2" align="left"/><thead><row rowsep="1" valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd">Classification techniques</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd">Application in a credit scoring context</entry></row></thead><tbody><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd">Logistic regression (LOG)</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd"><cross-refs refid="b0015 b0020 b0065 b0125 b0145 b0150">Arminger, Enache, and Bonne (1997), Baesens et al. (2003), Desai et al. (1996), Steenackers and Goovaerts (1989), West (2000), Wiginton (1980)</cross-refs></entry></row><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd" namest="col1" nameend="col2"><vsp sp="0.5"/></entry></row><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd">Decision trees (C4.5, CART, etc.)</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd"><cross-refs refid="b0015 b0020 b0145 b0165">Arminger et al. (1997), Baesens et al. (2003), West (2000), Yobas et al. (2000)</cross-refs></entry></row><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd" namest="col1" nameend="col2"><vsp sp="0.5"/></entry></row><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd">Neural networks (NN)</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd"><cross-refs refid="b0010 b0015 b0020 b0065 b0145 b0165">Altman (1994), Arminger et al. (1997), Baesens et al. (2003), Desai et al. (1996), West (2000), Yobas et al. (2000)</cross-refs></entry></row><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd" namest="col1" nameend="col2"><vsp sp="0.5"/></entry></row><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd">Linear discriminant analysis (LDA)</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd"><cross-refs refid="b0005 b0020 b0065 b0145 b0165">Altman (1968), Baesens et al. (2003), Desai et al. (1996), West (2000), Yobas et al. (2000)</cross-refs></entry></row><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd" namest="col1" nameend="col2"><vsp sp="0.5"/></entry></row><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd">Quadratic discriminant analysis (QDA)</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd"><cross-refs refid="b0005 b0020">Altman (1968), Baesens et al. (2003)</cross-refs></entry></row><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd" namest="col1" nameend="col2"><vsp sp="0.5"/></entry></row><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd"><italic>k</italic>-Nearest neighbours (<italic>k</italic>-NN)</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd"><cross-refs refid="b0020 b0045 b0145">Baesens et al. (2003), Chatterjee and Barcun (1970), West (2000)</cross-refs></entry></row><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd" namest="col1" nameend="col2"><vsp sp="0.5"/></entry></row><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd">Support vector machines (SVM, LS-SVM, etc.)</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd"><cross-refs refid="b0020 b0160">Baesens et al. (2003), Yang (2007)</cross-refs></entry></row></tbody></tgroup></ce:table><ce:table xmlns="http://www.elsevier.com/xml/common/cals/dtd" frame="topbot" id="t0010" rowsep="0" colsep="0"><ce:label>Table 2</ce:label><ce:caption><ce:simple-para id="sp050" view="all">Characteristics of credit scoring data sets.</ce:simple-para></ce:caption><tgroup cols="6"><colspec colname="col1" align="left"/><colspec colname="col2" align="char" char="."/><colspec colname="col3" align="char" char="."/><colspec colname="col4" align="char" char="."/><colspec colname="col5" align="char" char="."/><colspec colname="col6" align="left"/><thead><row rowsep="1" valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd"/><entry xmlns="http://www.elsevier.com/xml/common/dtd">Inputs</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd">Data set size</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd">Training set size</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd">Test set size</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd">Goods/bads</entry></row></thead><tbody><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd">Bene1</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">27</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">2974</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">1984</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">990</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd">70/30<cross-ref refid="tblfn1"><sup loc="post">⁎</sup></cross-ref></entry></row><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd">Bene2</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">27</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">7190</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">4795</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">2395</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd">70/30</entry></row><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd">Austr</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">14</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">547</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">366</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">181</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd">70/30<cross-ref refid="tblfn1"><sup loc="post">⁎</sup></cross-ref></entry></row><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd">Behav</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">60</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">1197</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">799</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">398</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd">70/30<cross-ref refid="tblfn1"><sup loc="post">⁎</sup></cross-ref></entry></row><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd">Germ</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">20</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">1000</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">668</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">332</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd">70/30</entry></row></tbody></tgroup><ce:table-footnote id="tblfn1"><ce:label>⁎</ce:label><ce:note-para>Altered data set class distribution, Bene1 original distribution was 66.6% good observations, 33.3% bad observations, Austr original distribution was 55.5% good observations, 44.5% bad observations and the Behav original distribution was 80% good observations, 20% bad observations.</ce:note-para></ce:table-footnote></ce:table><ce:table xmlns="http://www.elsevier.com/xml/common/cals/dtd" frame="topbot" id="t0015" rowsep="0" colsep="0"><ce:label>Table 3</ce:label><ce:caption><ce:simple-para id="sp055" view="all">Area under the receiver operating characteristic curve (AUC) results on test set data sets.</ce:simple-para></ce:caption><tgroup cols="19"><colspec colname="col1" align="left"/><colspec colname="col2" align="char" char="."/><colspec colname="col3" align="char" char="."/><colspec colname="col4" align="char" char="."/><colspec colname="col5" align="char" char="."/><colspec colname="col6" align="char" char="."/><colspec colname="col7" align="char" char="."/><colspec colname="col8" align="char" char="."/><colspec colname="col9" align="char" char="."/><colspec colname="col10" align="char" char="."/><colspec colname="col11" align="char" char="."/><colspec colname="col12" align="char" char="."/><colspec colname="col13" align="char" char="."/><colspec colname="col14" align="char" char="."/><colspec colname="col15" align="char" char="."/><colspec colname="col16" align="char" char="."/><colspec colname="col17" align="char" char="."/><colspec colname="col18" align="char" char="."/><colspec colname="col19" align="char" char="."/><thead><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd"/><entry xmlns="http://www.elsevier.com/xml/common/dtd" namest="col2" nameend="col7">30% Bad</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" namest="col8" nameend="col13">15% Bad</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" namest="col14" nameend="col19">10% Bad</entry></row><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd"/><entry xmlns="http://www.elsevier.com/xml/common/dtd" namest="col2" nameend="col7" rowsep="1">Friedman test statistic<hsp sp="0.25"/>=<hsp sp="0.25"/>31.86 (<italic>p</italic><hsp sp="0.25"/>&lt;<hsp sp="0.25"/>0.005)</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" namest="col8" nameend="col13" rowsep="1">Friedman test statistic<hsp sp="0.25"/>=<hsp sp="0.25"/>29.23 (<italic>p</italic><hsp sp="0.25"/>&lt;<hsp sp="0.25"/>0.005)</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" namest="col14" nameend="col19" rowsep="1">Friedman test statistic<hsp sp="0.25"/>=<hsp sp="0.25"/>26.37 (<italic>p</italic><hsp sp="0.25"/>&lt;<hsp sp="0.25"/>0.005)</entry></row><row rowsep="1" valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd"/><entry xmlns="http://www.elsevier.com/xml/common/dtd">Bene1</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd">Bene2</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd">Germ</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd">Aus</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd">Behav</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd">AR</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd">Bene1</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd">Bene2</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd">Germ</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd">Aus</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd">Behav</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd">AR</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd">Bene1</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd">Bene2</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd">Germ</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd">Aus</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd">Behav</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd">AR</entry></row></thead><tbody><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd">LOG</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">79.6</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">78.7</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">76.7</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">90.6</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">63.4</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">5.4</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">79.4</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">78.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">74.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">91.8</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">67.8</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">4.5</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">78.1</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">78.8</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">76.6</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">50.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">65.4</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">4.4</entry></row><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd">C4.5</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">71.4</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">71.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">71.2</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">91.8</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">61.9</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">9.1</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">69.7</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">60.9</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">65.2</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">91.6</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">61.6</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">8.2</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">64.7</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">64.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">64.1</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">91.9</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">50.3</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">8.4</entry></row><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd">NN</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">78.6</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">78.1</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">72.7</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">92.1</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">72.1</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">5.7</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">75.5</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">77.6</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">70.1</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">92.1</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">70.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">5.7</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">75.1</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">76.4</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">72.4</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">89.7</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">68.8</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">5.8</entry></row><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd">Gradient boosting</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">78.2</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="."><underline>81.2</underline></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">77.2</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">94.9</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">72.1</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">3.7</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="."><underline>79.8</underline></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="."><underline>80.3</underline></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">75.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="."><underline>94.8</underline></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">70.7</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="."><underline>2.3</underline></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">78.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="."><underline>80.2</underline></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">75.3</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">93.8</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">63.3</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">3.2</entry></row><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd">LDA</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">79.2</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">78.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">79.1</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">94.4</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">75.6</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">3.8</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">78.6</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">77.4</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">76.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">93.8</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">76.6</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">3.2</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">77.9</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">77.3</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">74.2</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="."><underline>94.5</underline></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">70.1</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">3.2</entry></row><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd">QDA</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">75.4</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">73.7</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">71.8</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">85.5</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">63.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">8.5</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">68.4</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">72.5</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">59.7</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">65.4</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">51.4</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">9.2</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">67.2</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">70.8</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">52.8</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">84.9</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">50.7</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">8.4</entry></row><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd">Random forests</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">78.5</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">79.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">80.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">93.7</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">76.2</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">3.2</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">77.9</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">78.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">76.9</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">94.1</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">76.5</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">2.7</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="."><underline>78.6</underline></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">76.9</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">77.2</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">93.2</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="."><underline>74.7</underline></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="."><underline>2.2</underline></entry></row><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd"><italic>k</italic>-NN10</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">76.2</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">71.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">75.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">92.8</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">61.8</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">7.7</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">75.5</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">68.1</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">71.7</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">90.3</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">58.7</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">7.9</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">70.4</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">64.4</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">68.8</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">92.5</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">56.3</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">6.8</entry></row><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd"><italic>k</italic>-NN100</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">75.4</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">73.9</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">79.3</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">93.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">56.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">6.7</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">75.8</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">73.6</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="."><underline>78.1</underline></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">92.6</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">62.9</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">4.6</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">75.3</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">72.9</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="."><underline>78.5</underline></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">92.3</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">61.7</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">4.6</entry></row><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd">Lin LS-SVM</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="."><underline>80.3</underline></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">80.6</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="."><underline>81.9</underline></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="."><underline>95.1</underline></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="."><underline>82.9</underline></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="."><underline>1.2</underline></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">50.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">54.4</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">75.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">91.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="."><underline>90.0</underline></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">6.7</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">50.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">50.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">76.8</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">90.6</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">50.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">8.0</entry></row><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd" namest="col1" nameend="col19"><vsp sp="0.5"/></entry></row></tbody></tgroup><tgroup cols="19"><colspec colname="col1" align="left"/><colspec colname="col2" align="char" char="."/><colspec colname="col3" align="char" char="."/><colspec colname="col4" align="char" char="."/><colspec colname="col5" align="char" char="."/><colspec colname="col6" align="char" char="."/><colspec colname="col7" align="char" char="."/><colspec colname="col8" align="char" char="."/><colspec colname="col9" align="char" char="."/><colspec colname="col10" align="char" char="."/><colspec colname="col11" align="char" char="."/><colspec colname="col12" align="char" char="."/><colspec colname="col13" align="char" char="."/><colspec colname="col14" align="char" char="."/><colspec colname="col15" align="char" char="."/><colspec colname="col16" align="char" char="."/><colspec colname="col17" align="char" char="."/><colspec colname="col18" align="char" char="."/><colspec colname="col19" align="char" char="."/><thead><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd"/><entry xmlns="http://www.elsevier.com/xml/common/dtd" namest="col2" nameend="col7">5% Bad</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" namest="col8" nameend="col13">2.5% Bad</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" namest="col14" nameend="col19">1% Bad</entry></row><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd"/><entry xmlns="http://www.elsevier.com/xml/common/dtd" namest="col2" nameend="col7" rowsep="1">Friedman test statistic<hsp sp="0.25"/>=<hsp sp="0.25"/>26.29 (<italic>p</italic><hsp sp="0.25"/>&lt;<hsp sp="0.25"/>0.005)</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" namest="col8" nameend="col13" rowsep="1">Friedman test statistic<hsp sp="0.25"/>=<hsp sp="0.25"/>27.43 (<italic>p</italic><hsp sp="0.25"/>&lt;<hsp sp="0.25"/>0.005)</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" namest="col14" nameend="col19" rowsep="1">Friedman test statistic<hsp sp="0.25"/>=<hsp sp="0.25"/>30.86 (<italic>p</italic><hsp sp="0.25"/>&lt;<hsp sp="0.25"/>0.005)</entry></row><row rowsep="1" valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd"/><entry xmlns="http://www.elsevier.com/xml/common/dtd">Bene1</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd">Bene2</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd">Germ</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd">Aus</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd">Behav</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd">AR</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd">Bene1</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd">Bene2</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd">Germ</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd">Aus</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd">Behav</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd">AR</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd">Bene1</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd">Bene2</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd">Germ</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd">Aus</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd">Behav</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd">AR</entry></row></thead><tbody><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd">LOG</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="."><underline>75.0</underline></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">75.4</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">75.7</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">50.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">50.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">5.5</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">72.7</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">73.9</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">55.1</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">50.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">50.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">6.3</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">50.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">64.7</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">50.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">50.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">50.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">7.7</entry></row><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd">C4.5</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">58.6</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">64.9</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">56.5</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">75.4</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">55.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">7.6</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">65.8</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">67.9</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">61.4</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">58.7</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">53.9</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">7.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">50.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">55.5</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">64.2</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">50.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">50.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">6.9</entry></row><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd">NN</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">68.4</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">70.7</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">68.3</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">89.4</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="."><underline>64.4</underline></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">5.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">71.2</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">70.2</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">59.2</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">70.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">62.3</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">4.6</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">50.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">62.5</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">54.2</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">86.7</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">54.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">5.6</entry></row><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd">Gradient boosting</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">70.8</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="."><underline>78.0</underline></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="."><underline>76.6</underline></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">93.1</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">52.7</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">3.4</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">68.1</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="."><underline>74.7</underline></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="."><underline>71.4</underline></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="."><underline>88.3</underline></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">55.6</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="."><underline>2.8</underline></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">58.1</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="."><underline>69.1</underline></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">59.4</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">74.5</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">51.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">3.5</entry></row><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd">LDA</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">74.1</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">76.1</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">73.8</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="."><underline>93.5</underline></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">63.5</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="."><underline>2.6</underline></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="."><underline>75.7</underline></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">72.2</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">62.6</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">81.8</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">60.6</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">3.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">50.2</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">69.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">58.3</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">86.8</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">54.6</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">3.4</entry></row><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd">QDA</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">63.8</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">72.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">50.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">59.7</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">50.5</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">7.9</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">66.5</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">65.3</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">50.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">51.6</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">50.5</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">8.3</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">50.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">50.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">50.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">52.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">50.7</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">7.9</entry></row><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd">Random forests</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">73.2</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">75.8</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">75.2</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">93.2</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">63.1</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">3.2</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">69.2</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">71.3</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">69.1</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">87.9</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="."><underline>68.7</underline></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">3.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">61.9</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">67.4</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="."><underline>67.1</underline></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="."><underline>90.1</underline></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="."><underline>60.0</underline></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="."><underline>1.6</underline></entry></row><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd"><italic>k</italic>-NN10</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">65.2</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">62.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">67.1</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">88.6</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">53.5</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">7.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">59.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">56.3</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">59.3</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">72.8</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">54.7</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">7.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">52.5</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">52.3</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">54.8</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">67.2</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">50.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">6.5</entry></row><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd"><italic>k</italic>-NN100</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">74.7</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">71.3</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">75.8</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">92.3</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">59.8</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">3.6</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">70.6</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">68.8</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">69.3</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">87.8</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">58.3</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">3.8</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="."><underline>67.2</underline></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">63.2</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">63.6</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">90.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">51.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">3.1</entry></row><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd">Lin LS-SVM</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">50.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">50.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">50.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">87.8</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">50.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">9.2</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">50.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">50.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">50.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">65.2</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">50.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">9.2</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">50.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">50.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">50.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">50.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">50.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">8.8</entry></row></tbody></tgroup></ce:table></ce:floats><head><ce:title>An experimental comparison of classification algorithms for imbalanced credit scoring data sets</ce:title><ce:author-group><ce:author><ce:given-name>Iain</ce:given-name><ce:surname>Brown</ce:surname><ce:cross-ref refid="cor1"><ce:sup loc="post">⁎</ce:sup></ce:cross-ref><ce:e-address type="email">i.brown@soton.ac.uk</ce:e-address></ce:author><ce:author><ce:given-name>Christophe</ce:given-name><ce:surname>Mues</ce:surname><ce:e-address type="email">C.Mues@soton.ac.uk</ce:e-address></ce:author><ce:affiliation><ce:textfn>School of Management, University of Southampton, Highfield, Southampton SO17 1BJ, UK</ce:textfn></ce:affiliation><ce:correspondence id="cor1"><ce:label>⁎</ce:label><ce:text>Corresponding author. Address: 44 Holters Mill, The Spires, Canterbury, Kent CT2 8SP, UK. Tel.: +44 (0) 7840057162.</ce:text></ce:correspondence></ce:author-group><ce:abstract class="author" xml:lang="en"><ce:section-title>Abstract</ce:section-title><ce:abstract-sec><ce:simple-para id="sp005" view="all">In this paper, we set out to compare several techniques that can be used in the analysis of imbalanced credit scoring data sets. In a credit scoring context, imbalanced data sets frequently occur as the number of defaulting loans in a portfolio is usually much lower than the number of observations that do not default. As well as using traditional classification techniques such as logistic regression, neural networks and decision trees, this paper will also explore the suitability of gradient boosting, least square support vector machines and random forests for loan default prediction.</ce:simple-para><ce:simple-para id="sp010" view="all">Five real-world credit scoring data sets are used to build classifiers and test their performance. In our experiments, we progressively increase class imbalance in each of these data sets by randomly under-sampling the minority class of defaulters, so as to identify to what extent the predictive power of the respective techniques is adversely affected. The performance criterion chosen to measure this effect is the area under the receiver operating characteristic curve (AUC); Friedman's statistic and Nemenyi post hoc tests are used to test for significance of AUC differences between techniques.</ce:simple-para><ce:simple-para id="sp015" view="all">The results from this empirical study indicate that the random forest and gradient boosting classifiers perform very well in a credit scoring context and are able to cope comparatively well with pronounced class imbalances in these data sets. We also found that, when faced with a large class imbalance, the C4.5 decision tree algorithm, quadratic discriminant analysis and <ce:italic>k</ce:italic>-nearest neighbours perform significantly worse than the best performing classifiers.</ce:simple-para></ce:abstract-sec></ce:abstract><ce:keywords class="keyword"><ce:section-title>Keywords</ce:section-title><ce:keyword><ce:text>Credit scoring</ce:text></ce:keyword><ce:keyword><ce:text>Imbalanced datasets</ce:text></ce:keyword><ce:keyword><ce:text>Classification</ce:text></ce:keyword><ce:keyword><ce:text>Benchmarking</ce:text></ce:keyword></ce:keywords></head><body view="all"><ce:sections><ce:section id="s0005" view="all"><ce:label>1</ce:label><ce:section-title>Introduction</ce:section-title><ce:para id="p0005" view="all">The aim of credit scoring is essentially to classify loan applicants into two classes, i.e., good payers (i.e., those who are likely to keep up with their repayments) and bad payers (i.e., those who are likely to default on their loans). In the current financial climate, and with the recent introduction of the Basel II Accord, financial institutions have even more incentives to select and implement the most appropriate credit scoring techniques for their credit portfolios. It is stated in <ce:cross-ref refid="b0090">Henley and Hand (1997)</ce:cross-ref> that companies could make significant future savings if an improvement of only a fraction of a percent could be made in the accuracy of the credit scoring techniques implemented. However, in the research literature, portfolios that can be considered as very low risk, or low default portfolios (LDPs), have had relatively little attention paid to them in particular with regards to which techniques are most appropriate for scoring them. The underlying problem with LDPs is that they contain a much smaller number of observations in the class of defaulters than in that of the good payers. A large class imbalance is therefore present which some techniques may not be able to successfully handle. Typical examples of low default portfolios include high-quality corporate borrowers, banks, sovereigns and some categories of specialised lending (<ce:cross-ref refid="b0135">Van Der Burgt, 2007</ce:cross-ref>) but in some countries even certain retail lending portfolios could turn out to have very low numbers of defaults compared to the majority class. In a recent FSA publication regarding conservative estimation of low default portfolios, regulatory concerns were raised about whether firms can adequately asses the risk of LDPs (<ce:cross-ref refid="b0030">Benjamin, Cathcart, &amp; Ryan, 2006</ce:cross-ref>).</ce:para><ce:para id="p0010" view="all">A wide range of classification techniques have already been proposed in the credit scoring literature, including statistical techniques, such as linear discriminant analysis and logistic regression, and non-parametric models, such as <ce:italic>k</ce:italic>-nearest neighbour and decision trees. But it is currently unclear from the literature which technique is the most appropriate for improving discrimination for LDPs. <ce:cross-ref refid="t0005">Table 1</ce:cross-ref><ce:float-anchor refid="t0005"/> provides a selection of techniques currently applied in a credit scoring context, along with references showing some of their reported applications in the literature.</ce:para><ce:para id="p0015" view="all">Hence, the aim of this paper is to conduct a study of various classification techniques based on five real-life credit scoring data sets. These data sets will then have the size of their minority class of defaulters further reduced by decrements of 5% (from an original 70/30 good/bad split) to see how the performance of the various classification techniques is affected by increasing class imbalance.</ce:para><ce:para id="p0020" view="all">The five real-life credit scoring data sets used in this empirical research study include two data sets from Benelux (Belgium, Netherlands and Luxembourg) institutions, the German Credit and Australian Credit data sets which are publicly available at the UCI repository (<ce:inter-ref xlink:href="http://www.kdd.ics.uci.edu/" xlink:type="simple">http://kdd.ics.uci.edu/</ce:inter-ref>), and the fifth data set is a behavioural scoring data set, which was also obtained from a Benelux institution.</ce:para><ce:para id="p0025" view="all">The techniques that will be applied in this paper are logistic regression (LOG), linear and quadratic discriminant analysis (LDA, QDA), least square support vector machines (LS-SVM), decision trees (C4.5), neural networks (NN), nearest-neighbour classifiers (<ce:italic>k</ce:italic>-NN10, <ce:italic>k</ce:italic>-NN100), a gradient boosting algorithm and random forests. We are especially interested in the power and usefulness of the gradient boosting and random forest classifiers which have yet to be thoroughly investigated in a credit scoring context.</ce:para><ce:para id="p0030" view="all">All techniques will be evaluated in terms of their area under the receiver operating characteristic curve (AUC). This is a measure of the discrimination power of a classifier without regard to class distribution or misclassification cost (<ce:cross-ref refid="b0020">Baesens et al., 2003</ce:cross-ref>).</ce:para><ce:para id="p0035" view="all">To make statistical inferences from the observed difference in AUC, we followed the recommendations given in a recent article (<ce:cross-ref refid="b0060">Demšar, 2006</ce:cross-ref>) that looked at the problem of benchmarking classifiers on multiple data sets. The recommendations given were for a set of simple robust non-parametric tests for the statistical comparison of the classifiers (<ce:cross-ref refid="b0060">Demšar, 2006</ce:cross-ref>). The AUC measures will therefore be compared using Friedman's average rank test, and Nemenyi's post hoc test will be employed to test the significance of the differences in rank between individual classifiers. Finally, a variant of Demšar's significance diagrams will be plotted to visualise their results.</ce:para><ce:para id="p0040" view="all">The organisation of this paper is as follows. Section <ce:cross-ref refid="s0010">2</ce:cross-ref> will begin by providing a literature review of the work that has been conducted on the topic of classification for imbalanced data sets. A brief explanation will then be given for the ten classification techniques to be used in the analysis of the data sets. Secondly, the empirical set up and criteria used for comparing the classification performance will be described. Thirdly, the results of our experiments are presented and discussed. Finally, conclusions will be drawn from the study and recommendations for further research work will be outlined.</ce:para></ce:section><ce:section id="s0010" view="all"><ce:label>2</ce:label><ce:section-title>Literature review</ce:section-title><ce:para id="p0045" view="all">A wide range of different classification techniques for scoring credit data sets has been proposed in the literature, a non-exhaustive list of which was provided earlier in <ce:cross-ref refid="t0005">Table 1</ce:cross-ref>. In addition, some benchmarking studies have been undertaken to empirically compare the performance of these various techniques (e.g., <ce:cross-ref refid="b0020">Baesens et al., 2003</ce:cross-ref>), but they did not focus specifically on how these techniques compare on heavily imbalanced samples, or to what extent any such comparison is affected by the issue of class imbalance. For example, in <ce:cross-ref refid="b0020">Baesens et al. (2003)</ce:cross-ref> seventeen techniques including both well-known techniques such as logistic regression and discriminant analysis and more advanced techniques such as least square support vector machines were compared on eight real-life credit scoring data sets. Although more complicated techniques such as radial basis function least square support vector machines (RBF LS-SVM) and neural networks (NN) yielded good performances in terms of AUC, simpler linear classifiers such as linear discriminant analysis (LDA) and logistic regression (LOG) also gave very good performances. However, there are often conflicting opinions when comparing the conclusions of studies promoting differing techniques. For example, in <ce:cross-ref refid="b0165">Yobas, Crook, and Ross (2000)</ce:cross-ref>, the authors found that linear discriminant analysis (LDA) outperformed neural networks in the prediction of loan default, whereas in <ce:cross-ref refid="b0065">Desai, Crook, and Overstreet (1996)</ce:cross-ref>, neural networks were reported to actually perform significantly better than LDA. Furthermore, many empirical studies only evaluate a small number of classification techniques on a single credit scoring data set. The data sets used in these empirical studies are also often far smaller and less imbalanced than those data sets used in practice. Hence, the issue of which classification technique to use for credit scoring, particularly with a small number of bad observations, remains a challenging problem (<ce:cross-ref refid="b0020">Baesens et al., 2003</ce:cross-ref>).</ce:para><ce:para id="p0050" view="all">The topic of which good/bad distribution is the most appropriate in classifying a data set has been discussed in some detail in the machine learning and data mining literature. In <ce:cross-ref refid="b0140">Weiss and Provost (2003)</ce:cross-ref> it was found that the naturally occurring class distributions in the 25 data sets looked at, often did not produce the best-performing classifiers. More specifically, based on the AUC measure (which was preferred over the use of the error rate), it was shown that the optimal class distribution should contain between 50% and 90% minority class examples within the training set. Alternatively, a progressive adaptive sampling strategy for selecting the optimal class distribution is proposed in <ce:cross-ref refid="b0115">Provost, Jensen, and Oates (1999)</ce:cross-ref>. Whilst this method of class adjustment can be very effective for large data sets, with adequate observations in the minority class of defaulters, in some low default portfolios there are only a very small number of loan defaults to begin with.</ce:para><ce:para id="p0055" view="all">Various kinds of techniques have been compared in the literature to try and ascertain the most effective way of overcoming a large class imbalance. <ce:cross-ref refid="b0050">Chawla, Bowyer, Hall, and Kegelmeyer (2002)</ce:cross-ref> proposed a synthetic minority over-sampling technique (SMOTE) which was applied to example data sets in fraud, telecommunications management, and detection of oil spills in satellite images. In <ce:cross-ref refid="b0100">Japkowicz (2000)</ce:cross-ref>, over-sampling and downsizing were compared to the author's own method of "learning by recognition" in order to determine the most effective technique. The findings, however, were inconclusive but demonstrated that both over-sampling the minority class and downsizing the majority class can be very effective. Subsequently, <ce:cross-ref refid="b0025">Batista (2004)</ce:cross-ref> identified ten alternative techniques in dealing with class imbalances and trialed them on thirteen data sets. The techniques chosen included a variety of under-sampling and over-sampling methods. Findings suggested that generally over-sampling methods provide more accurate results than under-sampling methods. Also, a combination of either SMOTE (<ce:cross-ref refid="b0050">Chawla et al., 2002</ce:cross-ref>) and Tomek links or SMOTE and ENN (a nearest-neighbour cleaning rule), were proposed.</ce:para></ce:section><ce:section id="s0015" view="all"><ce:label>3</ce:label><ce:section-title>Overview of classification techniques</ce:section-title><ce:para id="p0060" view="all">This study aims to compare the performance of a wide range of classification techniques within a credit scoring context, thereby assessing to what extent they are affected by increasing class imbalance. For the purpose of this study, ten classifiers have been selected which provide a balance between well-established credit scoring techniques such as logistic regression, decision trees and neural networks, and newly developed machine learning techniques such as least square support vector machines, gradient boosting and random forests. A brief explanation of each of the techniques applied in this paper is presented below.</ce:para><ce:section id="s0020" view="all"><ce:label>3.1</ce:label><ce:section-title>Logistic regression</ce:section-title><ce:para id="p0065" view="all">For this paper, we will be focusing on the binary response of whether a creditor turns out to be a good or bad payer (i.e., non-defaulter vs. defaulter). For this binary response model, the response variable, <ce:italic>y</ce:italic>, can take on one of two possible values; i.e., <ce:italic>y</ce:italic><ce:hsp sp="0.25"/>=<ce:hsp sp="0.25"/>0 if the customer is a bad payer, <ce:italic>y</ce:italic><ce:hsp sp="0.25"/>=<ce:hsp sp="0.25"/>1 if he/she is a good payer. Let us assume <ce:bold>x</ce:bold> is a column vector of <ce:italic>M</ce:italic> explanatory variables and <ce:italic>π</ce:italic><ce:hsp sp="0.25"/>=<ce:hsp sp="0.25"/><ce:italic>Pr</ce:italic>(<ce:italic>y</ce:italic><ce:hsp sp="0.25"/>=<ce:hsp sp="0.25"/>1|<ce:bold>x</ce:bold>) is the response probability to be modelled. The number of observations is denoted by <ce:italic>N</ce:italic>. The logistic regression model then takes the form:<ce:display><ce:formula id="e0010"><ce:label>(1)</ce:label><mml:math altimg="si1.gif" overflow="scroll"><mml:mtext>logit</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>π</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>≡</mml:mo><mml:mi mathvariant="normal">log</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>π</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mtext>,</mml:mtext></mml:math></ce:formula></ce:display>where <ce:italic>α</ce:italic> is the intercept parameter and <ce:italic>β<ce:sup loc="post">T</ce:sup></ce:italic> contains the variable coefficients (<ce:cross-ref refid="b0095">Hosmer &amp; Stanley, 2000</ce:cross-ref>).</ce:para></ce:section><ce:section id="s0025" view="all"><ce:label>3.2</ce:label><ce:section-title>Linear and quadratic discriminant analysis</ce:section-title><ce:para id="p0070" view="all">Discriminant analysis assigns an observation to the response, <mml:math altimg="si2.gif" overflow="scroll"><mml:mrow><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mn>0</mml:mn><mml:mtext>,</mml:mtext><mml:mn>1</mml:mn><mml:mo stretchy="false">}</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math>, with the largest posterior probability; i.e., classify into class 0 if <mml:math altimg="si3.gif" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&gt;</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math>, or class 1 if the reverse is true. According to Bayes' theorem, these posterior probabilities are given by<ce:display><ce:formula id="e0015"><ce:label>(2)</ce:label><mml:math altimg="si4.gif" overflow="scroll"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mtext>.</mml:mtext></mml:math></ce:formula></ce:display>Assuming now that the class-conditional distributions <ce:italic>p</ce:italic>(<ce:bold>x</ce:bold>|<ce:italic>y</ce:italic><ce:hsp sp="0.25"/>=<ce:hsp sp="0.25"/>0), <ce:italic>p</ce:italic>(<ce:bold>x</ce:bold>|<ce:italic>y</ce:italic><ce:hsp sp="0.25"/>=<ce:hsp sp="0.25"/>1) are multivariate normal distributions with mean vector <ce:italic>μ</ce:italic><ce:inf loc="post">0</ce:inf>, <ce:italic>μ</ce:italic><ce:inf loc="post">1</ce:inf>, and covariance matrix <ce:italic>Σ</ce:italic><ce:inf loc="post">0</ce:inf>, <ce:italic>Σ</ce:italic><ce:inf loc="post">1</ce:inf>, respectively, the classification rule becomes: classify as <ce:italic>y</ce:italic><ce:hsp sp="0.25"/>=<ce:hsp sp="0.25"/>0 if the following is satisfied:<ce:display><ce:formula id="e0020"><ce:label>(3)</ce:label><mml:math altimg="si5.gif" overflow="scroll"><mml:msup><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>2</mml:mn><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi mathvariant="normal">log</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>-</mml:mo><mml:mi mathvariant="normal">log</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mfenced></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="normal">log</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mrow><mml:mi>Σ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mo>-</mml:mo><mml:mi mathvariant="normal">log</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mrow><mml:mi>Σ</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo></mml:math></ce:formula></ce:display>Linear discriminant analysis is then obtained if the simplifying assumption is made that both covariance matrices are equal, i.e., <ce:italic>Σ</ce:italic><ce:inf loc="post">0</ce:inf><ce:hsp sp="0.25"/>=<ce:hsp sp="0.25"/><ce:italic>Σ</ce:italic><ce:inf loc="post">1</ce:inf><ce:hsp sp="0.25"/>=<ce:hsp sp="0.25"/><ce:italic>Σ</ce:italic>, which has the effect of cancelling out the quadratic terms in the expression above.</ce:para></ce:section><ce:section id="s0030" view="all"><ce:label>3.3</ce:label><ce:section-title>Neural networks (Multi-layer perceptron)</ce:section-title><ce:para id="p0075" view="all">Neural networks (NN) are mathematical representations modelled on the functionality of the human brain (<ce:cross-ref refid="b0035">Bishop, 1995</ce:cross-ref>). The added benefit of a NN is its flexibility in modelling virtually any non-linear association between input variables and target variable. Although various architectures have been proposed, our study focuses on probably the most widely used type of NN, i.e., the multilayer perceptron (MLP). A MLP is typically composed of an input layer (consisting of neurons for all input variables), a hidden layer (consisting of any number of hidden neurons), and an output layer (in our case, one neuron). Each neuron processes its inputs and transmits its output value to the neurons in the subsequent layer. Each such connection between neurons is assigned a weight during training. The output of hidden neuron <ce:italic>i</ce:italic> is computed by applying an activation function <ce:italic>f</ce:italic><ce:sup loc="post">(1)</ce:sup> (for example the logistic function) to the weighted inputs and its bias term <mml:math altimg="si6.gif" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math>:<ce:display><ce:formula id="e0025"><ce:label>(4)</ce:label><mml:math altimg="si7.gif" overflow="scroll"><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mtext>,</mml:mtext></mml:math></ce:formula></ce:display>where <ce:bold>W</ce:bold> represents a weight matrix in which <ce:bold>W</ce:bold><ce:italic><ce:inf loc="post">ij</ce:inf></ce:italic> denotes the weight connecting input <ce:italic>j</ce:italic> to hidden neuron <ce:italic>i</ce:italic>. For the analysis conducted in this paper, a binary prediction will be made; hence, for the activation function in the output layer, we will be using the logistic (sigmoid) activation function, <mml:math altimg="si8.gif" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math> to obtain a response probability:<ce:display><ce:formula id="e0030"><ce:label>(5)</ce:label><mml:math altimg="si9.gif" overflow="scroll"><mml:mi>π</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:msup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mtext>,</mml:mtext></mml:math></ce:formula></ce:display>with <ce:italic>n<ce:inf loc="post">h</ce:inf></ce:italic> the number of hidden neurons and <ce:bold>v</ce:bold> the weight vector where <ce:bold>v</ce:bold><ce:italic><ce:inf loc="post">j</ce:inf></ce:italic> represents the weight connecting hidden neuron <ce:italic>j</ce:italic> to the output neuron. During model estimation, the weights of the network are first randomly initialised and then iteratively adjusted so as to minimise an objective function, e.g., the sum of squared errors (possibly accompanied by a regularisation term to prevent over-fitting). This iterative procedure can be based on simple gradient descent learning or more sophisticated optimisation methods such as Levenberg-Marquardt or Quasi-Newton. The number of hidden neurons can be determined through a grid search based on validation set performance.</ce:para></ce:section><ce:section id="s0035" view="all"><ce:label>3.4</ce:label><ce:section-title>Least square support vector machines (LS-SVMs)</ce:section-title><ce:para id="p0080" view="all">Support vector machines (SVMs) are a set of powerful supervised learning techniques used for classification and regression. Their basic principle is to construct a maximum-margin separating hyperplane in some transformed feature space. Rather than requiring one to specify the exact transformation though, they use the principle of kernel substitution to turn them into a general (non-linear) model. The least square support vector machine (LS-SVM) proposed by <ce:cross-ref refid="b0130">Suykens, Van Gestel, De Brabanter, De Moor, and Vandewalle (2002)</ce:cross-ref> is a further adaptation of Vapnik's original SVM formulation which leads to solving linear KKT (Karush-Kuhn-Tucker) systems (rather than a more complex quadratic programing problem). The optimisation problem for the LS-SVM is defined as:<ce:display><ce:formula id="e0035"><ce:label>(6)</ce:label><mml:math altimg="si10.gif" overflow="scroll"><mml:munder><mml:mrow><mml:mi mathvariant="normal">min</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi><mml:mtext>,</mml:mtext><mml:mi>b</mml:mi><mml:mtext>,</mml:mtext><mml:mi mathvariant="bold">e</mml:mi></mml:mrow></mml:munder><mml:mi>J</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">w</mml:mi><mml:mtext>,</mml:mtext><mml:mi>b</mml:mi><mml:mtext>,</mml:mtext><mml:mi mathvariant="bold">e</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:msup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="bold">w</mml:mi><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mtext>,</mml:mtext></mml:math></ce:formula></ce:display>subject to the following equality constraints:<ce:display><ce:formula id="e0040"><ce:label>(7)</ce:label><mml:math altimg="si11.gif" overflow="scroll"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mfenced open="[" close="]"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">T</mml:mi></mml:mrow></mml:msup><mml:mi>φ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mtext>,</mml:mtext><mml:mspace width="1em"/><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mtext>,</mml:mtext><mml:mo>…</mml:mo><mml:mtext>,</mml:mtext><mml:mi>N</mml:mi><mml:mtext>,</mml:mtext></mml:math></ce:formula></ce:display>Where <ce:bold>w</ce:bold> is the weight vector in primal space, <ce:italic>γ</ce:italic> is the regularisation parameter, and <ce:italic>y<ce:inf loc="post">i</ce:inf></ce:italic><ce:hsp sp="0.25"/>=<ce:hsp sp="0.25"/>+1 or -1 for good (bad) payers, respectively (<ce:cross-ref refid="b0130">Suykens et al., 2002</ce:cross-ref>). A solution can then be obtained after constructing the Lagrangian, and choosing a particular kernel function <ce:italic>K</ce:italic>(<ce:bold>x</ce:bold>,<ce:hsp sp="0.12"/><ce:bold>x</ce:bold><ce:italic><ce:inf loc="post">i</ce:inf></ce:italic>) that computes inner products in the transformed space, based on which a classifier of the following form is obtained:<ce:display><ce:formula id="e0005"><mml:math altimg="si12.gif" overflow="scroll"><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mtext>sign</mml:mtext><mml:mrow><mml:mfenced open="[" close="]"><mml:mrow><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mtext><ce:italic>α</ce:italic></mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mtext>,</mml:mtext><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mtext>,</mml:mtext></mml:math></ce:formula></ce:display>where by <mml:math altimg="si13.gif" overflow="scroll"><mml:mrow><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mtext>,</mml:mtext><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>φ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>φ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math> is taken to be a positive definite kernel satisfying the Mercer theorem.The hyper parameter <ce:italic>γ</ce:italic> for the LS-SVM classification technique is tuned using 10-fold cross validation.</ce:para></ce:section><ce:section id="s0040" view="all"><ce:label>3.5</ce:label><ce:section-title>C4.5. decision trees</ce:section-title><ce:para id="p0085" view="all">A decision tree consists of internal nodes that specify tests on individual input variables or attributes that split the data into smaller subsets, and a series of leaf nodes assigning a class to each of the observations in the resulting segments. For our study, we chose the popular decision tree classifier C4.5, which builds decision trees using the concept of information entropy (<ce:cross-ref refid="b0120">Quinlan, 1993</ce:cross-ref>). The entropy of a sample <ce:italic>S</ce:italic> of classified observations is given by<ce:display><ce:formula id="e0045"><ce:label>(8)</ce:label><mml:math altimg="si14.gif" overflow="scroll"><mml:mi mathvariant="normal">Entropy</mml:mi><mml:mspace width="0.35em"/><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="normal">log</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="normal">log</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mtext>,</mml:mtext></mml:math></ce:formula></ce:display>where <ce:italic>p</ce:italic><ce:inf loc="post">1</ce:inf>(<ce:italic>p</ce:italic><ce:inf loc="post">0</ce:inf>) are the proportions of the class values 1(0) in the sample <ce:italic>S</ce:italic>, respectively. C4.5 examines the normalised information gain (entropy difference) that results from choosing an attribute for splitting the data. The attribute with the highest normalised information gain is the one used to make the decision. The algorithm then recurs on the smaller subsets.</ce:para></ce:section><ce:section id="s0045" view="all"><ce:label>3.6</ce:label><ce:section-title><ce:italic>k</ce:italic>-NN (memory based reasoning)</ce:section-title><ce:para id="p0090" view="all">The <ce:italic>k</ce:italic>-nearest neighbours algorithm (<ce:italic>k</ce:italic>-NN) classifies a data point by taking a majority vote of its <ce:italic>k</ce:italic> most similar data points (<ce:cross-ref refid="b0085">Hastie, Tibshirani, &amp; Friedman, 2001</ce:cross-ref>). The similarity measure used in this paper is the Euclidean distance between the two points:<ce:display><ce:formula id="e0050"><ce:label>(9)</ce:label><mml:math altimg="si15.gif" overflow="scroll"><mml:mi>d</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mtext>,</mml:mtext><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mo>=</mml:mo><mml:mo stretchy="false">‖</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">‖</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mfenced open="[" close="]"><mml:mrow><mml:msup><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mtext>.</mml:mtext></mml:math></ce:formula></ce:display></ce:para></ce:section><ce:section id="s0050" view="all"><ce:label>3.7</ce:label><ce:section-title>Random forests</ce:section-title><ce:para id="p0095" view="all">Random forests are defined as a group of un-pruned classification or regression trees, trained on bootstrap samples of the training data using random feature selection in the process of tree generation. After a large number of trees have been generated, each tree votes for the most popular class. These tree voting procedures are collectively defined as random forests. A more detailed explanation of how to train a random forest can be found in <ce:cross-ref refid="b0040">Breiman (2001)</ce:cross-ref>. For the Random Forests classification technique two parameters require tuning. These are the number of trees and the number of attributes used to grow each tree.</ce:para></ce:section><ce:section id="s0055" view="all"><ce:label>3.8</ce:label><ce:section-title>Gradient boosting</ce:section-title><ce:para id="p0100" view="all">Gradient boosting (<ce:cross-refs refid="b0075 b0080">Friedman, 2001, 2002</ce:cross-refs>) is an ensemble algorithm that improves the accuracy of a predictive function through incremental minimisation of the error term. After the initial base learner (most commonly a tree) is grown, each tree in the series is fit to the so-called "pseudo residuals" of the prediction from the earlier trees with the purpose of reducing the error. This leads to the following model:<ce:display><ce:formula id="e0055"><ce:label>(10)</ce:label><mml:math altimg="si16.gif" overflow="scroll"><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo>⋯</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mtext>,</mml:mtext></mml:math></ce:formula></ce:display>where <ce:italic>G</ce:italic><ce:inf loc="post">0</ce:inf> equals the first value for the series, <ce:italic>T</ce:italic><ce:inf loc="post">1</ce:inf>,<ce:hsp sp="0.12"/>…<ce:hsp sp="0.12"/>,<ce:hsp sp="0.12"/><ce:italic>T<ce:inf loc="post">n</ce:inf></ce:italic> are the trees fitted to the pseudo-residuals, and <ce:italic>β<ce:inf loc="post">i</ce:inf></ce:italic> are coefficients for the respective tree nodes computed by the gradient boosting algorithm. A more detailed explanation of gradient boosting can be found in <ce:cross-refs refid="b0075 b0080">Friedman (2001, 2002)</ce:cross-refs>. The gradient boosting classifier requires tuning of the number of iterations and the maximum branch size used in the splitting rule.</ce:para></ce:section></ce:section><ce:section id="s0060" view="all"><ce:label>4</ce:label><ce:section-title>Experimental set-up and data sets</ce:section-title><ce:section id="s0065" view="all"><ce:label>4.1</ce:label><ce:section-title>Data set characteristics</ce:section-title><ce:para id="p0105" view="all">The characteristics of the data sets used in evaluating the performance of the aforementioned classification techniques are given below in <ce:cross-ref refid="t0010">Table 2</ce:cross-ref><ce:float-anchor refid="t0010"/>. The Bene1 and Bene2 data sets were obtained from two major financial institutions in the Benelux region. For these two data sets, a bad customer was defined as someone who had missed three consecutive months of payments. The German credit data set and the Australian Credit data set are publicly available at the UCI repository (<ce:inter-ref xlink:href="http://www.kdd.ics.uci.edu/" xlink:type="simple">http://www.kdd.ics.uci.edu/</ce:inter-ref>). The Behav data set was also acquired from a Benelux institution. As all the data sets used have a reasonable number of observations they will each be split into a training (two thirds) and a test set (one third). This test set will remain unchanged throughout the analysis of the techniques.</ce:para></ce:section><ce:section id="s0070" view="all"><ce:label>4.2</ce:label><ce:section-title>Re-sampling setup and performance metrics</ce:section-title><ce:para id="p0110" view="all">In order for the percentage reduction in the bad observations, in each data set, to be relatively compared, the Bene1 set, Australian credit and the Behavioural Scoring set have first been altered to give a 70/30 class distribution. This was done by either under-sampling the bad observations (from a total of 1041 bad observations in the Bene1 data set, only 892 observations have been used; and from a total of 307 bad observations in the Australian credit data set, only 164 observations have been used) or under-sampling the good observations in the behavioural scoring data set, (from a total of 1436 good observations, only 838 observations have been used).</ce:para><ce:para id="p0115" view="all">For this empirical study, the class of defaulters in each of the training data sets was artificially reduced, by a factor of 5% up to 95% then by 2.5% and 1%, so as to create a larger difference in class distribution. As a result of this reduction, eight data sets were created for each of the five original data sets. The percentage splits created were 75%, 80%, 85%, 90%, 95%, 97.5%, 99% good observations. For this empirical study our focus is on the performance of classification techniques on data sets with a large class imbalance. Therefore detailed results will only be presented for the data set with the original 70/30 split, as a benchmark, and data sets with 85%, 90% and 99% splits. By doing so, it is possible to identify whether techniques are adversely affected in the prediction of the target variable when there is a substantially lower number of observations in one of the classes. The performance criterion chosen to measure this effect is the area under the receiver operator characteristic curve (AUC) statistic as proposed by <ce:cross-ref refid="b0020">Baesens et al. (2003)</ce:cross-ref>.</ce:para><ce:para id="p0120" view="all">The receiver operating characteristic curve (ROC) is a two-dimensional graphical illustration of the trade-off between the true positive rate (sensitivity) and false positive rate (1-specificity). The ROC curve illustrates the behaviour of a classifier without having to take into consideration the class distribution or misclassification cost. In order to compare the ROC curves of different classifiers, the area under the receiver operating characteristic curve (AUC) must be computed. The AUC statistic is similar to the Gini coefficient which is equal to 2<ce:hsp sp="0.25"/>×<ce:hsp sp="0.25"/>(<ce:italic>AUC</ce:italic><ce:hsp sp="0.25"/>-<ce:hsp sp="0.25"/>0.5). An example of an ROC curve is depicted in <ce:cross-ref refid="f0005">Fig. 1</ce:cross-ref><ce:float-anchor refid="f0005"/>:</ce:para><ce:para id="p0125" view="all">The diagonal line represents the trade-off between the sensitivity and (1-specificity) for a random model, and has an AUC of 0.5. For a well performing classifier the ROC curve needs to be as far to the top left-hand corner as possible. In the example shown in <ce:cross-ref refid="f0005">Fig. 1</ce:cross-ref>, the classifier that performs the best is the ROC<ce:inf loc="post">1</ce:inf> curve.</ce:para></ce:section><ce:section id="s0075" view="all"><ce:label>4.3</ce:label><ce:section-title>Parameter tuning and input selection</ce:section-title><ce:para id="p0130" view="all">The linear discriminant analysis (LDA), quadratic discriminant analysis (QDA) and logistic regression (LOG) classification techniques require no parameter tuning. The LOG model was built in SAS using proc logistic and using a stepwise variable selection method. Both the LDA and QDA techniques were run in SAS using proc discrim. Before all the techniques were run, dummy variables were created for the categorical variables. The AUC statistic was computed using the ROC macro by <ce:cross-ref refid="b0055">DeLong, DeLong, and Clarke-Pearson (1988)</ce:cross-ref>, which is available from the SAS website (<ce:inter-ref xlink:href="http://www.support.sas.com/kb/25/017.html" xlink:type="simple">http://.support.sas.com/kb/25/017.html</ce:inter-ref>).</ce:para><ce:para id="p0135" view="all">For the LS-SVM classifier, a linear kernel was chosen and a grid search mechanism was used to tune the hyper-parameters. For the LS-SVM, the LS-SVMlab Matlab toolbox developed by <ce:cross-ref refid="b0130">Suykens et al. (2002)</ce:cross-ref> was used.</ce:para><ce:para id="p0140" view="all">The NN classifiers were trained after selecting the best performing number of hidden neurons based on a validation set. The neural networks were trained in SAS Enterprise Miner using a logistic hidden and target layer activation function.</ce:para><ce:para id="p0145" view="all">The confidence level for the pruning strategy of C4.5 was varied from 0.01 to 0.5, and the most appropriate value was selected for each data set based on validation set performance. The tree was built using the Weka (<ce:cross-ref refid="b0155">Witten &amp; Frank, 2005</ce:cross-ref>) package.</ce:para><ce:para id="p0150" view="all">Two parameters have to be set for the Random Forests technique: these are the number of trees and the number of attributes used to grow each tree. A range of [10,<ce:hsp sp="0.12"/>50,<ce:hsp sp="0.12"/>100,<ce:hsp sp="0.12"/>250,<ce:hsp sp="0.12"/>500,<ce:hsp sp="0.12"/>1000] trees has been assessed, as well as three different settings for the number of randomly selected attributes per tree <mml:math altimg="si17.gif" overflow="scroll"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0.5</mml:mn><mml:mtext>,</mml:mtext><mml:mn>1</mml:mn><mml:mtext>,</mml:mtext><mml:mn>2</mml:mn><mml:mo stretchy="false">]</mml:mo><mml:mo>.</mml:mo><mml:msqrt><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msqrt><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math>, whereby <ce:italic>M</ce:italic> denotes the number of attributes within the respective data set (<ce:cross-ref refid="b0040">Breiman, 2001</ce:cross-ref>). As with the C4.5 algorithm, Random Forests were also trained in Weka (<ce:cross-ref refid="b0155">Witten &amp; Frank, 2005</ce:cross-ref>), using 10-fold cross-validation for tuning the parameters.</ce:para><ce:para id="p0155" view="all">The <ce:italic>k</ce:italic>-Nearest Neighbours technique was applied for both <ce:italic>k</ce:italic><ce:hsp sp="0.25"/>=<ce:hsp sp="0.25"/>10 and <ce:italic>k</ce:italic><ce:hsp sp="0.25"/>=<ce:hsp sp="0.25"/>100, using the Weka (<ce:cross-ref refid="b0155">Witten &amp; Frank, 2005</ce:cross-ref>) IBk classifier. For the gradient boosting classifier a partitioning algorithm was used as proposed by <ce:cross-ref refid="b0075">Friedman (2001)</ce:cross-ref>. The number of iterations was varied in the range [10,<ce:hsp sp="0.12"/>50,<ce:hsp sp="0.12"/>100,<ce:hsp sp="0.12"/>250,<ce:hsp sp="0.12"/>500,<ce:hsp sp="0.12"/>1000], with a maximum branch size of two selected for the splitting rule (<ce:cross-ref refid="b0075">Friedman, 2001</ce:cross-ref>). The gradient boosting node in SAS Enterprise Miner was used to run this technique.</ce:para></ce:section><ce:section id="s0080" view="all"><ce:label>4.4</ce:label><ce:section-title>Statistical comparison of classifiers</ce:section-title><ce:para id="p0160" view="all">We used Friedman's test (<ce:cross-ref refid="b0070">Friedman, 1940</ce:cross-ref>) to compare the AUCs of the different classifiers. The Friedman test statistic is based on the average ranked (AR) performances of the classification techniques on each data set, and is calculated as follows:<ce:display><ce:formula id="e0060"><ce:label>(11)</ce:label><mml:math altimg="si18.gif" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>χ</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>12</mml:mn><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>K</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mrow><mml:mfenced open="[" close="]"><mml:mrow><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mrow><mml:mi mathvariant="italic">AR</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>K</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:msup><mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow><mml:mtext>,</mml:mtext><mml:mspace width="1em"/><mml:mtext>where</mml:mtext><mml:mspace width="1em"/><mml:msub><mml:mrow><mml:mi mathvariant="italic">AR</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:mfrac><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:mtext>.</mml:mtext></mml:math></ce:formula></ce:display>In (13), <ce:italic>D</ce:italic> denotes the number of data sets used in the study, <ce:italic>K</ce:italic> is the total number of classifiers and <mml:math altimg="si19.gif" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math> is the rank of classifier <ce:italic>j</ce:italic> on data set <ce:italic>i</ce:italic>. <mml:math altimg="si20.gif" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>χ</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math> is distributed according to the Chi-square distribution with <ce:italic>K</ce:italic><ce:hsp sp="0.25"/>-<ce:hsp sp="0.25"/>1 degrees of freedom. If the value of <mml:math altimg="si21.gif" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>χ</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math> is large enough, then the null hypothesis that there is no difference between the techniques can be rejected. The Friedman statistic is well suited for this type of data analysis as it is less susceptible to outliers (<ce:cross-ref refid="b0070">Friedman, 1940</ce:cross-ref>).</ce:para><ce:para id="p0165" view="all">The post hoc Nemenyi test (<ce:cross-ref refid="b0110">Nemenyi, 1963</ce:cross-ref>) is applied to report any significant differences between individual classifiers. The Nemenyi post hoc test states that the performances of two or more classifiers are significantly different if their average ranks differ by at least the critical difference (CD), given by<ce:display><ce:formula id="e0065"><ce:label>(12)</ce:label><mml:math altimg="si22.gif" overflow="scroll"><mml:mi mathvariant="italic">CD</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mtext>,</mml:mtext><mml:mi>∞</mml:mi><mml:mtext>,</mml:mtext><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:msqrt><mml:mrow><mml:mfrac><mml:mrow><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>K</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>12</mml:mn><mml:mi>D</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:msqrt><mml:mtext>.</mml:mtext></mml:math></ce:formula></ce:display>In this formula, the value <ce:italic>q<ce:inf loc="post">α</ce:inf></ce:italic><ce:inf loc="post">,∞,</ce:inf><ce:italic><ce:inf loc="post">K</ce:inf></ce:italic> is based on the studentised range statistic (<ce:cross-ref refid="b0110">Nemenyi, 1963</ce:cross-ref>). Finally, the results from Friedman's statistic and the Nemenyi post hoc tests are displayed using a modified version of <ce:cross-ref refid="b0060">Demšar (2006)</ce:cross-ref> significance diagrams (<ce:cross-ref refid="b0105">Lessmann, Baesens, Mues, &amp; Pietsch, 2008</ce:cross-ref>). These diagrams display the ranked performances of the classification techniques along with the critical difference to clearly show any techniques which are significantly different to the best performing classifiers.</ce:para></ce:section></ce:section><ce:section id="s0085" view="all"><ce:label>5</ce:label><ce:section-title>Results and discussion</ce:section-title><ce:para id="p0170" view="all">The table on the following page (<ce:cross-ref refid="t0015">Table 3</ce:cross-ref><ce:float-anchor refid="t0015"/>) reports the AUCs of all ten classifiers on the five credit scoring data sets at varying degrees of class imbalance. For each level of imbalance, the Friedman test statistic and corresponding <ce:italic>p</ce:italic>-value is shown. As these were all significant (<ce:italic>p</ce:italic><ce:hsp sp="0.25"/>&lt;<ce:hsp sp="0.25"/>0.005) a post hoc Nemenyi test was then applied to each class distribution. The technique achieving the highest AUC on each data set is underlined as well as the overall highest ranked technique. <ce:cross-ref refid="t0015">Table 3</ce:cross-ref> shows that the gradient boosting algorithm has the highest Friedman score (average rank (AR)) on two of the five different percentage class splits. However at the extreme class split (99% good, 1% bad) Random Forests provides the best average ranking across the five data sets (Random Forests also ranks first on the 10% data set).</ce:para><ce:para id="p0175" view="all">In the majority of the class splits, the AR of the QDA and Lin LS-SVM classifiers are statistically worse than the AR of the Random Forests classifier at the 5% critical difference level (<ce:italic>α</ce:italic><ce:hsp sp="0.25"/>=<ce:hsp sp="0.25"/>0.05), as shown in the significance diagrams included next. Note that, even though the differences between the classifiers are small, it is important to note that in a credit scoring context, an increase in the discrimination ability of even a fraction of a percent may translate into significant future savings (<ce:cross-ref refid="b0090">Henley &amp; Hand, 1997</ce:cross-ref>).</ce:para><ce:para id="p0180" view="all">The following significance diagrams display the AUC performance ranks of the classifiers, along with Nemenyi's critical difference (CD) tail. The CD value for all the following diagrams is equal to 6.06. Each diagram shows the classification techniques listed in ascending order of ranked performance on the <ce:italic>y</ce:italic>-axis, and the classifier's mean rank across all five data sets displayed on the <ce:italic>x</ce:italic>-axis. Two vertical dashed lines have been inserted to clearly identify the end of the best performing classifier's tail and the start of the next significantly different classifier.</ce:para><ce:para id="p0185" view="all">The first significance diagram (see <ce:cross-ref refid="f0010">Fig. 2</ce:cross-ref><ce:float-anchor refid="f0010"/>) displays the average rank of the classifiers at the original class distribution of a 70% good, 30% bad split:</ce:para><ce:para id="p0190" view="all">At this original 70/30% split, the linear LS-SVM is the best performing classification technique with an AR value of 1.2. This diagram clearly shows that the <ce:italic>k</ce:italic>-NN10, QDA and C4.5 techniques perform significantly worse than the best performing classifier with values of 7.7, 8.5 and 9.1 respectively.</ce:para><ce:para id="p0195" view="all">The following significance diagram displays the average rank of the classifiers at an 85% good, 15% bad class split:</ce:para><ce:para id="p0200" view="all">At the level where only 15% of the data sets are bad observations, it is shown in the significance diagram that gradient boosting becomes the best performing classifier (see <ce:cross-ref refid="f0015">Fig. 3</ce:cross-ref><ce:float-anchor refid="f0015"/>). The gradient boosting classifier performs significantly better than the quadratic discriminant analysis (QDA) classifier. From these findings we can make a preliminary assumption that when a larger class imbalance is present, the QDA classifier remains significantly different to the gradient boosting classifier. All the other techniques used are not significantly different.</ce:para><ce:para id="p0245" view="all">At a 90% good, 10% bad class split the significance diagram shown in <ce:cross-ref refid="f0020">Fig. 4</ce:cross-ref><ce:float-anchor refid="f0020"/> indicates that the C4.5 and QDA algorithms are significantly worse than the random forests classifier. It can be noted that the Linear LS-SVM classifier however is progressively becoming less powerful as a large class imbalance is present (see <ce:cross-ref refid="f0025">Fig. 5</ce:cross-ref><ce:float-anchor refid="f0025"/>).</ce:para><ce:para id="p0210" view="all">The final split, displaying a 99% good, 1% bad class split, indicates that, at the most extreme class distribution analysed, two classification techniques are significantly worse (Lin LS-SVM and QDA). This displays an interesting finding that at the extreme split, LOG is now close to being significantly worse than the Random Forests algorithm. The logistic regression technique therefore shows limited power in correctly classifying observations where only a small number of bad observations exist. It can also be concluded that the random forests classifier performs surprisingly well given a large class imbalance.</ce:para><ce:para id="p0215" view="all">In summary, when considering the AUC performance measures, it can be concluded that the gradient boosting and random forest classifiers yield a very good performance at extreme levels of class imbalance, whereas the Lin LS-SVM sees a reduction in performance as a larger class imbalance is introduced. However, the simpler, linear classification techniques such as LDA and LOG also give a relatively good performance, which is not significantly different from that of the gradient boosting and random forest classifiers. This finding seems to confirm the suggestion made in <ce:cross-ref refid="b0020">Baesens et al. (2003)</ce:cross-ref> that most credit scoring data sets are only weakly non-linear. However, techniques such as QDA, C4.5 and <ce:italic>k</ce:italic>-NN10 perform significantly worse than the best performing classifiers at each percentage reduction. The majority of classification techniques yielded classification performances that are quite competitive with each other.</ce:para></ce:section><ce:section id="s0090" view="all"><ce:label>6</ce:label><ce:section-title>Conclusions and recommendations for further work</ce:section-title><ce:para id="p0220" view="all">In this comparative study we have looked at a number of credit scoring techniques, and studied their performance over various class distributions in five real-life credit data sets. Two techniques that have yet to be fully researched in the context of credit scoring, i.e., gradient boosting and random forests, were also chosen to give a broader review of the techniques available. The classification power of these techniques was assessed based on the area under the receiver operating characteristic curve (AUC). Friedman's test and Nemenyi's post hoc tests were then applied to determine whether the differences between the average ranked performances of the AUCs were statistically significant. Finally, these significance results were visualised using significance diagrams for each of the various class distributions analysed.</ce:para><ce:para id="p0225" view="all">The results of these experiments show that the gradient boosting and random forest classifiers performed well in dealing with samples where a large class imbalance was present. It does appear that in extreme cases the ability of random forests and gradient boosting to concentrate on 'local' features in the imbalanced data is useful. The most commonly used credit scoring techniques, linear discriminant analysis (LDA) and logistic regression (LOG), gave results that were reasonably competitive with the more complex techniques and this competitive performance continued even when the samples became much more imbalanced. This would suggest that the currently most popular approaches are fairly robust to imbalanced class sizes. On the other hand, techniques such as QDA and C4.5 were significantly worse than the best performing classifiers. It can also be concluded that the use of a linear kernel LS-SVM would not be beneficial in the scoring of data sets where a very large class imbalance exists.</ce:para><ce:para id="p0230" view="all">Further work that could be conducted, as a result of these findings, would be to firstly consider a stacking approach to classification through the combination of multiple techniques. Such an approach would allow a meta-learner to pick the best model to classify an observation. Secondly, another interesting extension to the research would be to apply these techniques on much larger data sets which display a wider variety of class distributions. It would also be of interest to look into the effect of not only the percentage class distribution but also the effect of the actual number of observations in a data set.</ce:para><ce:para id="p0235" view="all">Finally, as stated in the literature review section of this paper, there have been several approaches already researched in the area of over-sampling techniques to deal with large class imbalances. Further research into this and their effect on credit scoring model performance would be beneficial.</ce:para></ce:section></ce:sections><ce:acknowledgment><ce:section-title>Acknowledgements</ce:section-title><ce:para id="p0240" view="all">The authors of this paper would like to thank the <ce:grant-sponsor id="GS1" xlink:type="simple" xlink:role="http://www.elsevier.com/xml/linking-roles/grant-sponsor">EPSRC</ce:grant-sponsor> and <ce:grant-sponsor id="GS2" xlink:type="simple" xlink:role="http://www.elsevier.com/xml/linking-roles/grant-sponsor">SAS UK</ce:grant-sponsor> for their financial support to Iain Brown.</ce:para></ce:acknowledgment></body><tail view="all"><ce:bibliography id="bi0005" view="all"><ce:section-title>References</ce:section-title><ce:bibliography-sec id="bs0005"><ce:bib-reference id="b0005"><ce:label>Altman, 1968</ce:label><sb:reference><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>E.</ce:given-name><ce:surname>Altman</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Financial ratios, discriminant analysis and the prediction of corporate bankruptcy</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Journal of Finance</sb:maintitle></sb:title><sb:volume-nr>23</sb:volume-nr></sb:series><sb:issue-nr>4</sb:issue-nr><sb:date>1968</sb:date></sb:issue><sb:pages><sb:first-page>589</sb:first-page><sb:last-page>609</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="b0010"><ce:label>Altman, 1994</ce:label><sb:reference><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>E.</ce:given-name><ce:surname>Altman</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Corporate distress diagnosis: Comparisons using linear discriminant analysis and neural networks (the Italian Experience)</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Journal of Banking &amp; Finance</sb:maintitle></sb:title><sb:volume-nr>18</sb:volume-nr></sb:series><sb:issue-nr>3</sb:issue-nr><sb:date>1994</sb:date></sb:issue><sb:pages><sb:first-page>505</sb:first-page><sb:last-page>529</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="b0015"><ce:label>Arminger et al., 1997</ce:label><sb:reference><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>G.</ce:given-name><ce:surname>Arminger</ce:surname></sb:author><sb:author><ce:given-name>D.</ce:given-name><ce:surname>Enache</ce:surname></sb:author><sb:author><ce:given-name>T.</ce:given-name><ce:surname>Bonne</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Analyzing credit risk data: A comparison of logistic discrimination, classification tree analysis, and feed forward networks</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Computational Statistics</sb:maintitle></sb:title><sb:volume-nr>12</sb:volume-nr></sb:series><sb:date>1997</sb:date></sb:issue><sb:pages><sb:first-page>293</sb:first-page><sb:last-page>310</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="b0020"><ce:label>Baesens et al., 2003</ce:label><sb:reference><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>B.</ce:given-name><ce:surname>Baesens</ce:surname></sb:author><sb:author><ce:given-name>T.</ce:given-name><ce:surname>Van Gestel</ce:surname></sb:author><sb:author><ce:given-name>S.</ce:given-name><ce:surname>Viaene</ce:surname></sb:author><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Stepanova</ce:surname></sb:author><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Suykens</ce:surname></sb:author><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Vanthienen</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Benchmarking state-of-the-art classification algorithms for credit scoring</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Journal of the Operational Research Society</sb:maintitle></sb:title><sb:volume-nr>54</sb:volume-nr></sb:series><sb:issue-nr>6</sb:issue-nr><sb:date>2003</sb:date></sb:issue><sb:pages><sb:first-page>627</sb:first-page><sb:last-page>635</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="b0025"><ce:label>Batista, 2004</ce:label><sb:reference><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>G.</ce:given-name><ce:surname>Batista</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>A study of the behavior of several methods for balancing machine learning training data</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>ACM SIGKDD Explorations Newsletter</sb:maintitle></sb:title><sb:volume-nr>6</sb:volume-nr></sb:series><sb:issue-nr>1</sb:issue-nr><sb:date>2004</sb:date></sb:issue><sb:pages><sb:first-page>20</sb:first-page><sb:last-page>29</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="b0030"><ce:label>Benjamin et al., 2006</ce:label><ce:other-ref><ce:textref>Benjamin, N., Cathcart, A., &amp; Ryan, K. (2006). <ce:italic>Low default portfolios: A proposal for conservative estimation of default probabilities</ce:italic>. Discussion Paper. Financial Services Authority.</ce:textref></ce:other-ref></ce:bib-reference><ce:bib-reference id="b0035"><ce:label>Bishop, 1995</ce:label><sb:reference><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>C.M.</ce:given-name><ce:surname>Bishop</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Neural networks for pattern recognition</sb:maintitle></sb:title></sb:contribution><sb:host><sb:book><sb:date>1995</sb:date><sb:publisher><sb:name>Oxford University Press</sb:name><sb:location>Oxford, UK</sb:location></sb:publisher></sb:book></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="b0040"><ce:label>Breiman, 2001</ce:label><sb:reference><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>L.</ce:given-name><ce:surname>Breiman</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Random forests</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Machine Learning</sb:maintitle></sb:title><sb:volume-nr>45</sb:volume-nr></sb:series><sb:issue-nr>1</sb:issue-nr><sb:date>2001</sb:date></sb:issue><sb:pages><sb:first-page>5</sb:first-page><sb:last-page>32</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="b0045"><ce:label>Chatterjee and Barcun, 1970</ce:label><sb:reference><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>S.</ce:given-name><ce:surname>Chatterjee</ce:surname></sb:author><sb:author><ce:given-name>S.</ce:given-name><ce:surname>Barcun</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>A nonparametric approach to credit screening</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Journal of the American Statistical Association</sb:maintitle></sb:title><sb:volume-nr>65</sb:volume-nr></sb:series><sb:issue-nr>329</sb:issue-nr><sb:date>1970</sb:date></sb:issue><sb:pages><sb:first-page>50</sb:first-page><sb:last-page>154</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="b0050"><ce:label>Chawla et al., 2002</ce:label><sb:reference><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>N.V.</ce:given-name><ce:surname>Chawla</ce:surname></sb:author><sb:author><ce:given-name>K.W.</ce:given-name><ce:surname>Bowyer</ce:surname></sb:author><sb:author><ce:given-name>L.O.</ce:given-name><ce:surname>Hall</ce:surname></sb:author><sb:author><ce:given-name>W.P.</ce:given-name><ce:surname>Kegelmeyer</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>SMOTE: Synthetic Minority Over-sampling Technique</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Journal of Artificial Intelligence Research</sb:maintitle></sb:title><sb:volume-nr>16</sb:volume-nr></sb:series><sb:date>2002</sb:date></sb:issue><sb:pages><sb:first-page>321</sb:first-page><sb:last-page>357</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="b0055"><ce:label>DeLong et al., 1988</ce:label><sb:reference><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>E.R.</ce:given-name><ce:surname>DeLong</ce:surname></sb:author><sb:author><ce:given-name>D.M.</ce:given-name><ce:surname>DeLong</ce:surname></sb:author><sb:author><ce:given-name>D.L.</ce:given-name><ce:surname>Clarke-Pearson</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Comparing the areas under two or more correlated receiver operating characteristic curves: A nonparametric approach</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Biometrics</sb:maintitle></sb:title><sb:volume-nr>44</sb:volume-nr></sb:series><sb:issue-nr>3</sb:issue-nr><sb:date>1988</sb:date></sb:issue><sb:pages><sb:first-page>837</sb:first-page><sb:last-page>845</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="b0060"><ce:label>Demšar, 2006</ce:label><sb:reference><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Demšar</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Statistical comparisons of classifiers over multiple data sets</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Journal of Machine Learning Research</sb:maintitle></sb:title><sb:volume-nr>7</sb:volume-nr></sb:series><sb:date>2006</sb:date></sb:issue><sb:pages><sb:first-page>1</sb:first-page><sb:last-page>30</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="b0065"><ce:label>Desai et al., 1996</ce:label><sb:reference><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>V.S.</ce:given-name><ce:surname>Desai</ce:surname></sb:author><sb:author><ce:given-name>J.N.</ce:given-name><ce:surname>Crook</ce:surname></sb:author><sb:author><ce:given-name>G.A.</ce:given-name><ce:surname>Overstreet</ce:surname><ce:suffix>Jr.</ce:suffix></sb:author></sb:authors><sb:title><sb:maintitle>A comparison of neural networks and linear scoring models in the credit union environment</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>European Journal of Operational Research</sb:maintitle></sb:title><sb:volume-nr>95</sb:volume-nr></sb:series><sb:issue-nr>1</sb:issue-nr><sb:date>1996</sb:date></sb:issue><sb:pages><sb:first-page>24</sb:first-page><sb:last-page>37</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="b0070"><ce:label>Friedman, 1940</ce:label><sb:reference><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Friedman</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>A comparison of alternative tests of significance for the problem of m rankings</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Annals of Mathematical Statistics</sb:maintitle></sb:title><sb:volume-nr>11</sb:volume-nr></sb:series><sb:issue-nr>1</sb:issue-nr><sb:date>1940</sb:date></sb:issue><sb:pages><sb:first-page>86</sb:first-page><sb:last-page>92</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="b0075"><ce:label>Friedman, 2001</ce:label><sb:reference><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Friedman</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Greedy function approximation: A gradient boosting machine</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Annals of Statistics</sb:maintitle></sb:title><sb:volume-nr>29</sb:volume-nr></sb:series><sb:issue-nr>5</sb:issue-nr><sb:date>2001</sb:date></sb:issue><sb:pages><sb:first-page>1189</sb:first-page><sb:last-page>1232</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="b0080"><ce:label>Friedman, 2002</ce:label><sb:reference><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Friedman</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Stochastic gradient boosting</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Computational Statistics &amp; Data Analysis</sb:maintitle></sb:title><sb:volume-nr>38</sb:volume-nr></sb:series><sb:issue-nr>4</sb:issue-nr><sb:date>2002</sb:date></sb:issue><sb:pages><sb:first-page>367</sb:first-page><sb:last-page>378</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="b0085"><ce:label>Hastie et al., 2001</ce:label><sb:reference><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>T.</ce:given-name><ce:surname>Hastie</ce:surname></sb:author><sb:author><ce:given-name>R.</ce:given-name><ce:surname>Tibshirani</ce:surname></sb:author><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Friedman</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>The elements of statistical learning, data mining, inference, and prediction</sb:maintitle></sb:title></sb:contribution><sb:host><sb:book><sb:date>2001</sb:date><sb:publisher><sb:name>Springer</sb:name><sb:location>New York</sb:location></sb:publisher></sb:book></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="b0090"><ce:label>Henley and Hand, 1997</ce:label><sb:reference><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>W.E.</ce:given-name><ce:surname>Henley</ce:surname></sb:author><sb:author><ce:given-name>D.J.</ce:given-name><ce:surname>Hand</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Construction of a <ce:italic>k</ce:italic>-nearest neighbour credit-scoring system</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>IMA Journal of Management Mathematics</sb:maintitle></sb:title><sb:volume-nr>8</sb:volume-nr></sb:series><sb:issue-nr>4</sb:issue-nr><sb:date>1997</sb:date></sb:issue><sb:pages><sb:first-page>305</sb:first-page><sb:last-page>321</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="b0095"><ce:label>Hosmer and Stanley, 2000</ce:label><sb:reference><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>D.W.</ce:given-name><ce:surname>Hosmer</ce:surname></sb:author><sb:author><ce:given-name>L.</ce:given-name><ce:surname>Stanley</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Applied logistic regression</sb:maintitle></sb:title></sb:contribution><sb:host><sb:book><sb:edition>2nd ed.</sb:edition><sb:date>2000</sb:date><sb:publisher><sb:name>Wiley</sb:name><sb:location>Chichester, New York</sb:location></sb:publisher></sb:book></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="b0100"><ce:label>Japkowicz, 2000</ce:label><ce:other-ref><ce:textref>Japkowicz, N. (2000). Learning from imbalanced data sets: A comparison of various strategies. In <ce:italic>AAAI workshop on learning from imbalanced data sets</ce:italic> (Vol. 6, pp. 10-15).</ce:textref></ce:other-ref></ce:bib-reference><ce:bib-reference id="b0105"><ce:label>Lessmann et al., 2008</ce:label><sb:reference><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>S.</ce:given-name><ce:surname>Lessmann</ce:surname></sb:author><sb:author><ce:given-name>B.</ce:given-name><ce:surname>Baesens</ce:surname></sb:author><sb:author><ce:given-name>C.</ce:given-name><ce:surname>Mues</ce:surname></sb:author><sb:author><ce:given-name>S.</ce:given-name><ce:surname>Pietsch</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Benchmarking classification models for software defect prediction: A proposed framework and novel findings</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>IEEE Transactions on Software Engineering</sb:maintitle></sb:title><sb:volume-nr>34</sb:volume-nr></sb:series><sb:issue-nr>4</sb:issue-nr><sb:date>2008</sb:date></sb:issue><sb:pages><sb:first-page>485</sb:first-page><sb:last-page>496</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="b0110"><ce:label>Nemenyi, 1963</ce:label><ce:other-ref><ce:textref>Nemenyi, P. B. (1963). <ce:italic>Distribution-free multiple comparisons</ce:italic>. Ph.D. Thesis. Princeton University.</ce:textref></ce:other-ref></ce:bib-reference><ce:bib-reference id="b0115"><ce:label>Provost et al., 1999</ce:label><sb:reference><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>F.</ce:given-name><ce:surname>Provost</ce:surname></sb:author><sb:author><ce:given-name>D.</ce:given-name><ce:surname>Jensen</ce:surname></sb:author><sb:author><ce:given-name>T.</ce:given-name><ce:surname>Oates</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Efficient progressive sampling</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of the fifth international conference on knowledge discovery and data mining</sb:maintitle></sb:title><sb:date>1999</sb:date><sb:publisher><sb:name>ACM Press</sb:name></sb:publisher></sb:edited-book></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="b0120"><ce:label>Quinlan, 1993</ce:label><sb:reference><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>J.R.</ce:given-name><ce:surname>Quinlan</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>C4.5 programs for machine learning</sb:maintitle></sb:title></sb:contribution><sb:host><sb:book><sb:date>1993</sb:date><sb:publisher><sb:name>Morgan Kaufmann</sb:name><sb:location>San Mateo, CA</sb:location></sb:publisher></sb:book></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="b0125"><ce:label>Steenackers and Goovaerts, 1989</ce:label><sb:reference><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>A.</ce:given-name><ce:surname>Steenackers</ce:surname></sb:author><sb:author><ce:given-name>M.J.</ce:given-name><ce:surname>Goovaerts</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>A credit scoring model for personal loans</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Insurance: Mathematics and Economics</sb:maintitle></sb:title><sb:volume-nr>8</sb:volume-nr></sb:series><sb:issue-nr>1</sb:issue-nr><sb:date>1989</sb:date></sb:issue><sb:pages><sb:first-page>31</sb:first-page><sb:last-page>34</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="b0130"><ce:label>Suykens et al., 2002</ce:label><sb:reference><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>J.A.K.</ce:given-name><ce:surname>Suykens</ce:surname></sb:author><sb:author><ce:given-name>T.</ce:given-name><ce:surname>Van Gestel</ce:surname></sb:author><sb:author><ce:given-name>J.</ce:given-name><ce:surname>De Brabanter</ce:surname></sb:author><sb:author><ce:given-name>B.</ce:given-name><ce:surname>De Moor</ce:surname></sb:author><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Vandewalle</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Least squares support vector machines</sb:maintitle></sb:title></sb:contribution><sb:host><sb:book><sb:date>2002</sb:date><sb:publisher><sb:name>World Scientific</sb:name><sb:location>Singapore</sb:location></sb:publisher></sb:book></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="b0135"><ce:label>Van Der Burgt, 2007</ce:label><sb:reference><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Van Der Burgt</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Calibrating low-default portfolios, using the cumulative accuracy profile</sb:maintitle></sb:title></sb:contribution><sb:host><sb:book><sb:date>2007</sb:date><sb:publisher><sb:name>ABN AMRO</sb:name></sb:publisher></sb:book></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="b0140"><ce:label>Weiss and Provost, 2003</ce:label><sb:reference><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>G.M.</ce:given-name><ce:surname>Weiss</ce:surname></sb:author><sb:author><ce:given-name>F.J.</ce:given-name><ce:surname>Provost</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Learning when training data are costly: The effect of class distribution on tree induction</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Journal of Artificial Intelligence Research</sb:maintitle></sb:title><sb:volume-nr>19</sb:volume-nr></sb:series><sb:date>2003</sb:date></sb:issue><sb:pages><sb:first-page>315</sb:first-page><sb:last-page>354</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="b0145"><ce:label>West, 2000</ce:label><sb:reference><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>D.</ce:given-name><ce:surname>West</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Neural network credit scoring models</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Computers &amp; Operations Research</sb:maintitle></sb:title><sb:volume-nr>27</sb:volume-nr></sb:series><sb:issue-nr>11-12</sb:issue-nr><sb:date>2000</sb:date></sb:issue><sb:pages><sb:first-page>1131</sb:first-page><sb:last-page>1152</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="b0150"><ce:label>Wiginton, 1980</ce:label><sb:reference><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>J.C.</ce:given-name><ce:surname>Wiginton</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>A note on the comparison of logit and discriminant models of consumer credit behavior</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Journal of Financial and Quantitative Analysis</sb:maintitle></sb:title><sb:volume-nr>15</sb:volume-nr></sb:series><sb:date>1980</sb:date></sb:issue><sb:pages><sb:first-page>757</sb:first-page><sb:last-page>770</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="b0155"><ce:label>Witten and Frank, 2005</ce:label><sb:reference><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>I.H.</ce:given-name><ce:surname>Witten</ce:surname></sb:author><sb:author><ce:given-name>E.</ce:given-name><ce:surname>Frank</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Data mining: Practical machine learning tools and techniques</sb:maintitle></sb:title></sb:contribution><sb:host><sb:book><sb:edition>2nd ed.</sb:edition><sb:date>2005</sb:date><sb:publisher><sb:name>Morgan Kaufmann</sb:name><sb:location>San Francisco</sb:location></sb:publisher></sb:book></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="b0160"><ce:label>Yang, 2007</ce:label><sb:reference><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>Y.</ce:given-name><ce:surname>Yang</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Adaptive credit scoring with kernel learning methods</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>European Journal of Operational Research</sb:maintitle></sb:title><sb:volume-nr>183</sb:volume-nr></sb:series><sb:issue-nr>3</sb:issue-nr><sb:date>2007</sb:date></sb:issue><sb:pages><sb:first-page>1521</sb:first-page><sb:last-page>1536</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="b0165"><ce:label>Yobas et al., 2000</ce:label><sb:reference><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>M.B.</ce:given-name><ce:surname>Yobas</ce:surname></sb:author><sb:author><ce:given-name>J.N.</ce:given-name><ce:surname>Crook</ce:surname></sb:author><sb:author><ce:given-name>P.</ce:given-name><ce:surname>Ross</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Credit scoring using neural and evolutionary techniques</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>IMA Journal of Management Mathematics</sb:maintitle></sb:title><sb:volume-nr>11</sb:volume-nr></sb:series><sb:issue-nr>2</sb:issue-nr><sb:date>2000</sb:date></sb:issue><sb:pages><sb:first-page>111</sb:first-page><sb:last-page>125</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference></ce:bibliography-sec></ce:bibliography></tail></article></xocs:serial-item></xocs:doc>
